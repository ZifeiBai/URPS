{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# 1Ô∏è‚É£ Google Drive\n","drive.mount('/content/drive')\n","\n"],"metadata":{"id":"2EettNg4TW5f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738699356417,"user_tz":300,"elapsed":20089,"user":{"displayName":"Zifei Bai","userId":"12413018286570323363"}},"outputId":"3fba3e80-9ef2-48e7-8fdf-c54eb9fd85f8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WmNzSf047AQ"},"outputs":[],"source":["import math\n","import inspect\n","from dataclasses import dataclass\n","import numpy as np\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","source":["vocab = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '+', '&', '*']\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","padding_token_index = 13\n","end_token_index = 12"],"metadata":{"id":"nBnXU-eyOwC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a mapping from chars to ints\n","stoi = {ch:i for i, ch in enumerate(vocab)}\n","itos = {i:ch for i, ch in enumerate(vocab)}\n","encode = lambda s:[stoi[c] for c in s] # encoder: take a string, output a list of ints\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of ints, output a string\n","\n","print(encode(\"1+2=3&\"))\n","print(decode(encode(\"1+2=3&\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e4EbjUWPBjj","executionInfo":{"status":"ok","timestamp":1738698869312,"user_tz":300,"elapsed":83,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"outputId":"bae9d6a7-0e8d-4b98-9c4c-3dedeefc1e61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 11, 2, 10, 3, 12]\n","1+2=3&\n"]}]},{"cell_type":"code","source":["def get_batch(num_digits, batch_size=32, block_size=256):\n","    # pick 0-9 in 1-digit addition\n","    a = np.random.randint(10**(num_digits-1), 10**num_digits, batch_size)\n","    b = np.random.randint(10**(num_digits-1), 10**num_digits, batch_size)\n","    c = a + b\n","\n","    online_data = []\n","    val_list = []\n","    for i, j, k in zip(a, b, c):\n","        online_data.append(np.array(encode(f'{i}+{j}={k}&')))\n","        # val_list.append(f'{i}+{j}={k}&')\n","\n","    data = [np.pad(row, (0, block_size - len(row)), mode='constant', constant_values=encode('*')[0]) for row in online_data]\n","    data = np.array(data)\n","\n","    x_list = []\n","    y_list = []\n","\n","    for arr in data:\n","        # 1. Extract first 6 elements for x\n","        x_values = arr[:2+2*num_digits]  # Get first 6 elements\n","        x_list.append(torch.tensor(x_values, dtype=torch.int64))\n","\n","        first_13_index = np.where(arr == 13)[0]\n","        # If 13 is found, proceed with slicing\n","        if len(first_13_index) > 0:\n","            y_values = arr[2+2*num_digits:first_13_index[0]]  # Take elements before first 13 after '='\n","        else:\n","            y_values = arr[2+2*num_digits+1:]  # If no 13, take full array\n","        y_list.append(torch.tensor(y_values, dtype=torch.int64))\n","\n","    # 3. Pad x to (32, 256) using padding_value=13\n","    x_tensor = torch.nn.utils.rnn.pad_sequence(x_list, batch_first=True, padding_value=13)\n","\n","    # Ensure x_tensor is exactly (32, 256)\n","    if x_tensor.shape[1] < block_size:\n","        x_padding = torch.full((batch_size, block_size - x_tensor.shape[1]), 13, dtype=torch.int64)\n","        x_tensor = torch.cat((x_tensor, x_padding), dim=1)\n","\n","    x_tensor = x_tensor[:, :block_size]  # Ensure fixed size (32, 256)\n","\n","    # 4. Pad y to (32, max_len) using padding_value=13\n","    y_tensor = torch.nn.utils.rnn.pad_sequence(y_list, batch_first=True, padding_value=13)\n","\n","    # Ensure y_tensor is exactly (32, 256)\n","    if y_tensor.shape[1] < block_size:\n","        y_padding = torch.full((batch_size, block_size - y_tensor.shape[1]), 13, dtype=torch.int64)\n","        y_tensor = torch.cat((y_tensor, y_padding), dim=1)\n","\n","\n","    # print(\"x shape:\", x_tensor.shape)  # Expected output: (32, 256)\n","    # print(\"y shape:\", y_tensor.shape)  # Expected output: (32, 256)\n","\n","\n","    # return x.to(device), y.to(device)\n","    return x_tensor.to(device), y_tensor.to(device)\n","    # return val_list, x_tensor.to(device), y_tensor.to(device)"],"metadata":{"id":"yQPUNSWqZO2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_batch(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Fbr7rBey6SD","executionInfo":{"status":"ok","timestamp":1738698942498,"user_tz":300,"elapsed":109,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"outputId":"8fc8bad0-1c16-4862-e3ad-51d5efc1e710"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 5, 11,  6,  ..., 13, 13, 13],\n","         [ 8, 11,  3,  ..., 13, 13, 13],\n","         [ 5, 11,  8,  ..., 13, 13, 13],\n","         ...,\n","         [ 1, 11,  5,  ..., 13, 13, 13],\n","         [ 3, 11,  5,  ..., 13, 13, 13],\n","         [ 3, 11,  2,  ..., 13, 13, 13]]),\n"," tensor([[ 1,  1, 12,  ..., 13, 13, 13],\n","         [ 1,  1, 12,  ..., 13, 13, 13],\n","         [ 1,  3, 12,  ..., 13, 13, 13],\n","         ...,\n","         [ 6, 12, 13,  ..., 13, 13, 13],\n","         [ 8, 12, 13,  ..., 13, 13, 13],\n","         [ 5, 12, 13,  ..., 13, 13, 13]]))"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["import math\n","import inspect\n","from dataclasses import dataclass\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","class LayerNorm(nn.Module):\n","    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n","\n","    def __init__(self, ndim, bias):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n","\n","class CausalSelfAttention(nn.Module):\n","\n","    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n","        super().__init__()\n","        # Á°Æ‰øù n_embd ÂèØ‰ª•Ë¢´ n_head Êï¥Èô§Ôºå ÊØè‰∏™Â§¥Áª¥Â∫¶Áõ∏Á≠â\n","        assert n_embd % n_head == 0\n","        # key, query, value projections for all heads, but in a batch\n","        # ‰∏Ä‰∏™linearÂ±ÇÔºåÂêåÊó∂ËÆ°ÁÆóQ, K, V; ËæìÂá∫Áª¥Â∫¶ÊòØÔºàB, T, 3 * n_embdÔºâ\n","        # ËæìÂÖ•ÁöÑxÊòØ(B, T, n_embd)\n","        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n","        # output projection\n","        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n","        # regularization\n","        self.attn_dropout = nn.Dropout(dropout)\n","        self.resid_dropout = nn.Dropout(dropout)\n","        self.n_head = n_head\n","        self.n_embd = n_embd\n","        self.dropout = dropout\n","        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        if not self.flash:\n","            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n","            # causal mask to ensure that attention is only applied to the left in the input sequence\n","            self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n","                                        .view(1, 1, block_size, block_size))\n","\n","    def forward(self, x, padding_mask=None):\n","        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n","\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        #tensor.split(a = ÊãÜÊàêÂ§öÂ§ß‰∏Ä‰∏™Ôºåb = Âú®Âì™‰∏™Áª¥Â∫¶‰∏äÊãÜ)\n","        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n","        # view(B, T, self.n_head, C // self.n_head) ÊãÜÂàÜÂ§öÂ§¥\n","        # n_embd Ë¢´ÊãÜÂàÜÊàê n_head ‰ªΩÔºåÊØè‰∏™Â§¥ÁöÑÁª¥Â∫¶ÊòØ C // n_head\n","        # (B, T, n_embd) ‚Üí (B, T, n_head, head_dim)\n","        # transpose(1, 2) ‰∫§Êç¢Áª¥Â∫¶\n","        # ‰∫§Êç¢ TÔºàÂ∫èÂàóÈïøÂ∫¶ÔºâÂíå n_head\n","        # (B, T, n_head, head_dim) -> (B, n_head, T, head_dim)\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","\n","        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n","        if self.flash:\n","            # efficient attention using Flash Attention CUDA kernels\n","            # Ëøô‰∏™ÂáΩÊï∞Â∞±ÊòØÂÆûÁé∞Ê†áÂáÜÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ËøêÁÆó\n","            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n","        else:\n","            # manual implementation of attention\n","            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n","\n","            if padding_mask is not None:\n","                att = att.masked_fill(padding_mask[:, None, None, :T] == 0, float('-inf'))\n","\n","            att = F.softmax(att, dim=-1)\n","            att = self.attn_dropout(att)\n","            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n","        # transpose(1, 2)ÁöÑ‰ΩúÁî®Ôºö (B, n_head, T, head_dim) ‚Üí (B, T, n_head, head_dim)\n","        # contiguous() ËÆ©Âº†ÈáèÂú®ÂÜÖÂ≠ò‰∏≠ËøûÁª≠Â≠òÂÇ®Ôºå‰ª•‰æø view() Ê≠£Â∏∏ËøêË°å\n","        # view(B, T, C) ÂêàÂπ∂ n_head Âíå head_dimÔºåÂèòÂõû n_embd\n","        # (B, T, n_head, head_dim) ‚Üí (B, T, n_embd)\n","        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n","\n","        # output projection\n","        y = self.resid_dropout(self.c_proj(y))\n","        return y\n","\n","class MLP(nn.Module):\n","\n","    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n","        super().__init__()\n","        # ‰ΩúÁî®ÔºöÂ∞Ü ËæìÂÖ•ÁöÑ 768 Áª¥ÂêëÈáèÊâ©Â±ïÂà∞ 3072 Áª¥ÔºåÁõ∏ÂΩì‰∫éÂ¢ûÂä†‰∫ÜÁâπÂæÅÁª¥Â∫¶\n","        # ÁõÆÁöÑÔºöÊõ¥È´òÁª¥Â∫¶ÁöÑÁâπÂæÅÁ©∫Èó¥Ôºå‰ΩøÊ®°ÂûãÊõ¥ÂÆπÊòìÂ≠¶‰π†Â§çÊùÇÂÖ≥Á≥ª\n","        self.c_fc = nn.Linear(n_embd, 4 * n_embd, bias=bias)\n","        # ÊøÄÊ¥ªÂáΩÊï∞ GELUÔºàGaussian Error Linear UnitÔºâ\n","        # GELU ÊòØ ReLU ÁöÑÊîπËøõÁâàÔºåÂ∏∏Áî®‰∫é Transformer Ê®°Âûã\n","        self.gelu = nn.GELU()\n","        # ÂèòÂõû 768 Áª¥\n","        self.c_proj  = nn.Linear(4 * n_embd, n_embd, bias=bias)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.c_fc(x) # (B, T, 768) -> (B, T, 3072)\n","        x = self.gelu(x) # GELU ÊøÄÊ¥ªÂáΩÊï∞\n","        x = self.c_proj(x) # (B, T, 3072) -> (B, T, 768)\n","        x = self.dropout(x)\n","        return x\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n","        super().__init__()\n","        self.ln_1 = LayerNorm(n_embd, bias=bias)\n","        self.attn = CausalSelfAttention(n_embd, n_head, dropout, block_size, bias=True)\n","        self.ln_2 = LayerNorm(n_embd, bias=bias)\n","        self.mlp = MLP(n_embd, n_head, dropout, block_size, bias=True)\n","\n","    def forward(self, x, padding_mask=None):\n","        x = x + self.attn(self.ln_1(x), padding_mask=padding_mask)\n","        x = x + self.mlp(self.ln_2(x))\n","        return x\n","\n","\n","class GPT(nn.Module):\n","\n","    def __init__(self, vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=True):\n","        super().__init__()\n","        assert vocab_size is not None\n","        assert block_size is not None\n","        self.vocab_size = vocab_size\n","        self.block_size = block_size\n","        self.n_embd = n_embd\n","        self.n_layer = n_layer\n","        self.n_head = n_head\n","        self.dropout = dropout\n","        self.bias = bias\n","\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(vocab_size, n_embd), # token embeddings\n","            wpe = nn.Embedding(block_size, n_embd), # positional embeddings\n","            drop = nn.Dropout(dropout),\n","            h = nn.ModuleList([Block(n_embd, n_head, dropout, block_size, bias=bias) for _ in range(n_layer)]), # a stack of n_layer blocks\n","            ln_f = LayerNorm(n_embd, bias=bias), # final layer norm\n","        ))\n","        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False) # projects the final transformer output to the vocab size\n","\n","        # init all weights\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None, padding_mask=None):\n","        device = idx.device\n","        b, t = idx.size()\n","        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.cblock_size}\"\n","        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n","\n","        # forward the GPT model itself\n","        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n","        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n","        x = self.transformer.drop(tok_emb + pos_emb)\n","        for block in self.transformer.h:\n","            x = block(x, padding_mask=padding_mask)\n","        x = self.transformer.ln_f(x)\n","\n","        logits = self.lm_head(x)  # Compute logits outside the if block\n","        loss = None\n","\n","        if targets is not None:\n","\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=13)\n","            # inference-time mini-optimization: only forward the lm_head on the very last position\n","            # logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n","\n","        return logits, loss\n","\n","    @torch.no_grad()\n","    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n","        \"\"\"\n","        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n","        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n","        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n","        \"\"\"\n","        for _ in range(max_new_tokens):\n","            # if the sequence context is growing too long we must crop it at block_size\n","            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n","            # forward the model to get the logits for the index in the sequence\n","            logits, _ = self(idx_cond)\n","            # pluck the logits at the final step and scale by desired temperature\n","            logits = logits[:, -1, :] / temperature\n","            # optionally crop the logits to only the top k options\n","            if top_k is not None:\n","                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","                logits[logits < v[:, [-1]]] = -float('Inf')\n","            # apply softmax to convert logits to (normalized) probabilities\n","            probs = F.softmax(logits, dim=-1)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","            # append sampled index to the running sequence and continue\n","            idx = torch.cat((idx, idx_next), dim=1)\n","\n","        return idx"],"metadata":{"id":"XVIg1aizaT7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_iters = 200\n","\n","@torch.no_grad()\n","def estimate_loss(num_digits, padding_mask):\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","      losses = torch.zeros(eval_iters)\n","      for k in range(eval_iters):\n","          X, Y = get_batch(num_digits)\n","          logits, loss = model(X, Y, padding_mask)\n","          losses[k] = loss.item()\n","      out[split] = losses.mean()\n","    model.train()\n","    return out"],"metadata":{"id":"cNC6DqTgcbZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32 # how many independent sequences will we process in parallel?\n","block_size = 256 # what is the maximum context length for predictions?\n","max_iters = 1000\n","num_epochs = 1\n","eval_interval = 100\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 20\n","n_embd = 256\n","n_head = 8\n","n_layer = 6\n","dropout = 0.1\n","# # torch.manual_seed(1337)\n","# if torch.cuda.is_available():\n","#     torch.cuda.manual_seed_all(1337)\n","bias = True # if using bias inside all Linear layers\n","block_size = 256\n","vocab_size = len(vocab)"],"metadata":{"id":"xOdwJ26Ax2e6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=bias)\n","m = model.to(device)"],"metadata":{"id":"DPl7iF7e2D-n","executionInfo":{"status":"error","timestamp":1738648341895,"user_tz":300,"elapsed":285,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"colab":{"base_uri":"https://localhost:8080/","height":356},"outputId":"db1d823d-c81f-40b4-edc4-ef6e595fd540"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-228-0e2bbce5a0a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"code","source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"],"metadata":{"id":"snZoFt8qe76q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"n0mUgmCpOlEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","for epoch in range(num_epochs):\n","\n","    for iter in tqdm(range(max_iters), desc=\"Processing\"):\n","        # sample a batch of data\n","        xb, yb = get_batch(epoch+1)\n","        padding_mask_x = (xb != padding_token_index).long()\n","\n","        if iter % 100 == 0:\n","          losses = estimate_loss(epoch+1, padding_mask_x)\n","          print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","        # evaluate the loss\n","        logits, loss = model(xb, yb, padding_mask_x)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","\n","    scheduler.step()  # Ë∞ÉÊï¥Â≠¶‰π†Áéá\n"],"metadata":{"id":"odtqfBE0swtz","colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"status":"error","timestamp":1738648170657,"user_tz":300,"elapsed":16,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"outputId":"26f2bebd-cbe5-48e6-cecb-5f793ee81023"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.811776 M parameters\n"]},{"output_type":"stream","name":"stderr","text":["Processing:   0%|          | 0/1000 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-221-f5c625998e5a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-192-96e9ec5e29bb>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m(num_digits, padding_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_digits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m           \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-217-3fb1ec3c9737>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets, padding_mask)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m# inference-time mini-optimization: only forward the lm_head on the very last position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# note: using list [-1] to preserve the time dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"code","source":["# early_stopping\n","# accuracy function"],"metadata":{"id":"sEKyA9IlOe5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assume you have encode() and decode() functions\n","input_str = \"12+34=\"\n","\n","# Step 1: Encode string into token indices\n","input_tokens = torch.tensor(encode(input_str), dtype=torch.long, device=device).unsqueeze(0)  # Shape (1, t)\n","\n","# Step 2: Generate new tokens\n","output_tokens = model.generate(input_tokens, max_new_tokens=500)\n","\n","# Step 3: Decode token indices back into a string\n","output_str = decode(output_tokens[0].tolist())\n","\n","# Step 4: Print the generated result\n","print(output_str)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xRXry9lmqEN","executionInfo":{"status":"ok","timestamp":1738647986350,"user_tz":300,"elapsed":2293,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"outputId":"e8ebe32c-7e7f-4f1a-f46e-4330046fa107"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12+34=6\n","6\n","\n","\n","\n","\n","4\n","\n","\n","\n","6\n","\n","\n","344\n","\n","\n","\n","5\n","\n","\n","4\n","43\n","2\n","6\n","\n","\n","\n","\n","\n","2\n","\n","6\n","*60\n","3\n","2\n","55+6\n","+\n","+\n","\n","\n","=\n","3\n","\n","3\n","9+\n","503+\n","=147\n","00\n","5\n","6\n","7\n","438\n","\n","\n","\n","\n","=46\n","\n","0\n","+=79\n","=\n","\n","3\n","*\n","71\n","729\n","2\n","3\n","*\n","+\n","\n","2+06\n","9\n","+248\n","758\n","\n","4\n","46\n","6\n","6\n","5372\n","\n","\n","=5\n","276\n","23\n","\n","6756\n","5=\n","86\n","7\n","2\n","\n","\n","\n","8\n","1\n","58\n","*53\n","68469\n","\n","\n","=2*62+484*+\n","2++\n","\n","5\n","\n","\n","94\n","\n","468\n","=\n","4427\n","136\n","=952\n","\n","=\n","*85\n","66*88=\n","6=183\n","7385=\n","2\n","\n","598\n","6\n","096\n","7\n","\n","29*4=\n","\n","\n","67\n","905\n","0\n","3\n","\n","\n","58\n","9398709*779\n","=789738\n","6\n","0926\n","\n","\n","=4338\n","8+6\n","\n","*+6\n","27\n","\n","98326\n","9*3+2\n","1\n","3\n","=92*8\n","92*239+464\n","83*9=\n","5\n","1\n","682\n","1656*\n","7\n","98\n","9269+3\n","\n","446619*3\n","\n","86\n","6448+++\n","=71=\n","794\n","\n","416\n","*\n","1=6+28+\n","59\n","1345876\n","\n"]}]},{"cell_type":"code","source":["import subprocess\n","\n","os.system('git config --global user.email \"zifeibai@umich.edu\"')\n","os.system('git config --global user.name \"ZifeiBai\"')\n","\n","# 2Ô∏è‚É£ **‰ΩøÁî® Google Drive Â≠òÂÇ® GitHub Token**\n","GITHUB_TOKEN_PATH = \"/content/drive/MyDrive/URPS/github_token.txt\"\n","if os.path.exists(GITHUB_TOKEN_PATH):\n","    with open(GITHUB_TOKEN_PATH, \"r\") as f:\n","        os.environ[\"GITHUB_TOKEN\"] = f.read().strip()\n","else:\n","    print(\"‚ùå GitHub Token\")\n","    exit(1)\n","\n","# 3Ô∏è‚É£ **ËÆæÁΩÆ GitHub ËøúÁ®ã‰ªìÂ∫ì**\n","GIT_PATH = \"/content/drive/MyDrive/URPS/Git\"\n","REPO_URL = f\"https://{os.environ['GITHUB_TOKEN']}@github.com/ZifeiBai/URPS.git\"\n","\n","if not os.path.exists(GIT_PATH):\n","    print(f\"üìÅ Creating directory: {GIT_PATH}\")\n","    os.makedirs(GIT_PATH)\n","\n","# 4Ô∏è‚É£ **Â¶ÇÊûú .git/ ÁõÆÂΩï‰∏çÂ≠òÂú®ÔºåËØ¥Êòé‰∏çÊòØ Git ‰ªìÂ∫ìÔºåÈúÄË¶ÅÂÖãÈöÜ**\n","if not os.path.exists(os.path.join(GIT_PATH, \".git\")):\n","    print(\"‚ùå Git repository not found. Cloning...\")\n","    subprocess.run(f\"rm -rf {GIT_PATH}\", shell=True, check=True)\n","    subprocess.run(f\"git clone {REPO_URL} {GIT_PATH}\", shell=True, check=True)\n","\n","# 5Ô∏è‚É£ **ËøõÂÖ• Git ÁõÆÂΩï**\n","os.chdir(GIT_PATH)\n","print(\"üìÇ Changed working directory to:\", os.getcwd())\n","\n","# # 6Ô∏è‚É£ **Â§çÂà∂Êñá‰ª∂**\n","# SOURCE_FILE = \"/content/drive/MyDrive/URPS/Colabs/transformer.ipynb\"\n","# if os.path.exists(SOURCE_FILE):\n","#     subprocess.run(f\"cp {SOURCE_FILE} {GIT_PATH}/\", shell=True, check=True)\n","#     print(f\"‚úÖ Copied {SOURCE_FILE} to {GIT_PATH}\")\n","# else:\n","#     print(\"‚ùå transformer.ipynb not found!\")\n","#     exit(1)\n","\n","\n","# 7Ô∏è‚É£ **Ê£ÄÊü• Git Áä∂ÊÄÅ**\n","status_output = subprocess.run(\"git status\", shell=True, capture_output=True, text=True)\n","print(status_output.stdout)\n","\n","# 8Ô∏è‚É£ **Êèê‰∫§Êõ¥ÊîπÂπ∂Êé®ÈÄÅ**\n","print(\"üöÄ Adding files to Git...\")\n","subprocess.run(\"git add .\", shell=True, check=True)\n","\n","print(\"üìù Committing changes...\")\n","commit_output = subprocess.run('git commit -m \"Auto update from Google Colab\"', shell=True, capture_output=True, text=True)\n","print(commit_output.stdout)\n","\n","\n","\n","print(\"üì§ Pushing to GitHub...\")\n","push_output = subprocess.run(\"git push origin main\", shell=True, capture_output=True, text=True)\n","if \"fatal\" in push_output.stderr or \"error:\" in push_output.stderr:\n","    print(\"‚ùå Real Git Push Error:\", push_output.stderr)\n","else:\n","    print(\"‚úÖ Git Push Success! (Warnings ignored)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23jaqlhaaOGQ","executionInfo":{"status":"ok","timestamp":1738555361247,"user_tz":300,"elapsed":31693,"user":{"displayName":"Zifei Bai","userId":"12413018286570323363"}},"outputId":"600aa6e9-98d2-4866-e624-c7734e80cf9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üìÇ Changed working directory to: /content/drive/MyDrive/URPS/Git\n","On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\tmodified:   transformer.ipynb\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n","\n","üöÄ Adding files to Git...\n","üìù Committing changes...\n","[main 045b55e] Auto update from Google Colab\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite transformer.ipynb (96%)\n","\n","üì§ Pushing to GitHub...\n","‚úÖ Git Push Success! (Warnings ignored)\n"]}]}]}