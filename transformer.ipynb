{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# 1Ô∏è‚É£ ÊåÇËΩΩ Google Drive\n","drive.mount('/content/drive')\n","\n"],"metadata":{"id":"2EettNg4TW5f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738545595585,"user_tz":300,"elapsed":19754,"user":{"displayName":"Zifei Bai","userId":"12413018286570323363"}},"outputId":"74db4600-da04-4de1-c697-362bd36785e4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import subprocess\n","\n","# 2Ô∏è‚É£ **‰ΩøÁî® Google Drive Â≠òÂÇ® GitHub Token**\n","GITHUB_TOKEN_PATH = \"/content/drive/MyDrive/URPS/github_token.txt\"\n","if os.path.exists(GITHUB_TOKEN_PATH):\n","    with open(GITHUB_TOKEN_PATH, \"r\") as f:\n","        os.environ[\"GITHUB_TOKEN\"] = f.read().strip()\n","else:\n","    print(\"‚ùå GitHub Token Êñá‰ª∂‰∏çÂ≠òÂú®ÔºåËØ∑Âú® Google Drive ÂàõÂª∫ 'github_token.txt' Âπ∂Â≠òÂÖ•‰Ω†ÁöÑ TokenÔºÅ\")\n","    exit(1)\n","\n","# 3Ô∏è‚É£ **ËÆæÁΩÆ GitHub ËøúÁ®ã‰ªìÂ∫ì**\n","GIT_PATH = \"/content/drive/MyDrive/URPS/Git\"\n","REPO_URL = f\"https://{os.environ['GITHUB_TOKEN']}@github.com/ZifeiBai/URPS.git\"\n","\n","if not os.path.exists(GIT_PATH):\n","    print(f\"üìÅ Creating directory: {GIT_PATH}\")\n","    os.makedirs(GIT_PATH)\n","\n","# 4Ô∏è‚É£ **Â¶ÇÊûú .git/ ÁõÆÂΩï‰∏çÂ≠òÂú®ÔºåËØ¥Êòé‰∏çÊòØ Git ‰ªìÂ∫ìÔºåÈúÄË¶ÅÂÖãÈöÜ**\n","if not os.path.exists(os.path.join(GIT_PATH, \".git\")):\n","    print(\"‚ùå Git repository not found. Cloning...\")\n","    subprocess.run(f\"rm -rf {GIT_PATH}\", shell=True, check=True)\n","    subprocess.run(f\"git clone {REPO_URL} {GIT_PATH}\", shell=True, check=True)\n","\n","# 5Ô∏è‚É£ **ËøõÂÖ• Git ÁõÆÂΩï**\n","os.chdir(GIT_PATH)\n","print(\"üìÇ Changed working directory to:\", os.getcwd())\n","\n","# 6Ô∏è‚É£ **Â§çÂà∂Êñá‰ª∂**\n","SOURCE_FILE = \"/content/drive/MyDrive/URPS/Colabs/transformer.ipynb\"\n","if os.path.exists(SOURCE_FILE):\n","    subprocess.run(f\"cp {SOURCE_FILE} {GIT_PATH}/\", shell=True, check=True)\n","    print(f\"‚úÖ Copied {SOURCE_FILE} to {GIT_PATH}\")\n","else:\n","    print(\"‚ùå transformer.ipynb not found!\")\n","    exit(1)\n","\n","# 7Ô∏è‚É£ **Ê£ÄÊü• Git Áä∂ÊÄÅ**\n","status_output = subprocess.run(\"git status\", shell=True, capture_output=True, text=True)\n","print(status_output.stdout)\n","\n","# 8Ô∏è‚É£ **Êèê‰∫§Êõ¥ÊîπÂπ∂Êé®ÈÄÅ**\n","print(\"üöÄ Adding files to Git...\")\n","subprocess.run(\"git add .\", shell=True, check=True)\n","\n","print(\"üìù Committing changes...\")\n","commit_output = subprocess.run('git commit -m \"Auto update from Google Colab\"', shell=True, capture_output=True, text=True)\n","print(commit_output.stdout)\n","\n","print(\"üîÑ Pulling latest changes...\")\n","subprocess.run(\"git pull origin main --rebase\", shell=True, check=True)\n","\n","print(\"üì§ Pushing to GitHub...\")\n","push_output = subprocess.run(\"git push origin main\", shell=True, capture_output=True, text=True)\n","print(push_output.stdout)\n","print(\"‚ö†Ô∏è Push Error:\", push_output.stderr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23jaqlhaaOGQ","executionInfo":{"status":"ok","timestamp":1738547532381,"user_tz":300,"elapsed":2829,"user":{"displayName":"Zifei Bai","userId":"12413018286570323363"}},"outputId":"fe3bc1ff-0f89-4466-b6f7-3be80499e6ca"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["üìÇ Changed working directory to: /content/drive/MyDrive/URPS/Git\n","‚úÖ Copied /content/drive/MyDrive/URPS/Colabs/transformer.ipynb to /content/drive/MyDrive/URPS/Git\n","On branch main\n","Your branch is ahead of 'origin/main' by 4 commits.\n","  (use \"git push\" to publish your local commits)\n","\n","nothing to commit, working tree clean\n","\n","üöÄ Adding files to Git...\n","üìù Committing changes...\n","On branch main\n","Your branch is ahead of 'origin/main' by 4 commits.\n","  (use \"git push\" to publish your local commits)\n","\n","nothing to commit, working tree clean\n","\n","üîÑ Pulling latest changes...\n","üì§ Pushing to GitHub...\n","\n","‚ö†Ô∏è Push Error: remote: error: GH013: Repository rule violations found for refs/heads/main.        \n","remote: \n","remote: - GITHUB PUSH PROTECTION        \n","remote:   ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî        \n","remote:     Resolve the following violations before pushing again        \n","remote: \n","remote:     - Push cannot contain secrets        \n","remote: \n","remote:             \n","remote:      (?) Learn how to resolve a blocked push        \n","remote:      https://docs.github.com/code-security/secret-scanning/working-with-secret-scanning-and-push-protection/working-with-push-protection-from-the-command-line#resolving-a-blocked-push        \n","remote:             \n","remote:             \n","remote:       ‚Äî‚Äî GitHub Personal Access Token ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî        \n","remote:        locations:        \n","remote:          - commit: 4304e76064089695bed624c0eae4329f5c4ec26b        \n","remote:            path: transformer.ipynb:1        \n","remote:          - commit: 4304e76064089695bed624c0eae4329f5c4ec26b        \n","remote:            path: transformer.ipynb:1        \n","remote:             \n","remote:        (?) To push, remove secret from commit(s) or follow this URL to allow the secret.        \n","remote:        https://github.com/ZifeiBai/URPS/security/secret-scanning/unblock-secret/2sVhb58E7MjqvauijqEHnXBHsRp        \n","remote:             \n","remote: \n","remote: \n","To https://github.com/ZifeiBai/URPS.git\n"," ! [remote rejected] main -> main (push declined due to repository rule violations)\n","error: failed to push some refs to 'https://github.com/ZifeiBai/URPS.git'\n","\n"]}]},{"cell_type":"code","source":["!git status  # Á°Æ‰øùÂ∑•‰ΩúÂå∫Âπ≤ÂáÄ"],"metadata":{"id":"KpIu0K_7hE9a","executionInfo":{"status":"ok","timestamp":1738547680638,"user_tz":300,"elapsed":289,"user":{"displayName":"Zifei Bai","userId":"12413018286570323363"}},"outputId":"c3760353-bc14-41c9-9545-311c12788e8d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is ahead of 'origin/main' by 4 commits.\n","  (use \"git push\" to publish your local commits)\n","\n","nothing to commit, working tree clean\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"V_XejFFQhI__"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WmNzSf047AQ"},"outputs":[],"source":["import math\n","import inspect\n","from dataclasses import dataclass\n","import numpy as np\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"code","source":[],"metadata":{"id":"El_1ZYYRO3Go"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Pp90QmgONyNE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738543337063,"user_tz":300,"elapsed":1092,"user":{"displayName":"Zifei Bai","userId":"12413018286570323363"}},"outputId":"8419902b-a690-4b3c-e368-126d21394653"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["with open('/content/drive/My Drive/raw_data_100k.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","print(text[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FjI2VT16MQ3_","executionInfo":{"status":"ok","timestamp":1738273514983,"user_tz":300,"elapsed":7,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"outputId":"62eee039-a1b8-4de1-9976-064dd1f9562e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["80 + 80 = 160\n","86 + 97 = 183\n","43 + 14 = 57\n","20 + 39 = 59\n","54 + 62 = 116\n","0 + 39 = 39\n","50 + 49 = 99\n","52 + 16 = 68\n","24 + 93 = 117\n","15 + 2 = 17\n","0 + 44 = 44\n","61 + 51 = 112\n","3 + 23 = 26\n","95 + 10 = 105\n","51 + 60 = 111\n","58 + 73 = 131\n","74 + 48 = 122\n","23 + 54 = 77\n","13 + 51 = 64\n","97 + 62 = 159\n","9 + 92 = 101\n","25 + 35 = 60\n","46 + 6 = 52\n","64 + 56 = 120\n","47 + 72 = 119\n","22 + 32 = 54\n","27 + 61 = 88\n","21 + 83 = 104\n","99 + 90 = 189\n","98 + 43 = 141\n","38 + 66 = 104\n","27 + 54 = 81\n","89 + 4 = 93\n","3 + 46 = 49\n","79 + 34 = 113\n","33 + 90 = 123\n","78 + 43 = 121\n","94 + 21 = 115\n","22 + 34 = 56\n","13 + 40 = 53\n","77 + 53 = 130\n","0 + 67 = 67\n","74 + 72 = 146\n","36 + 89 = 125\n","91 + 51 = 142\n","75 + 0 = 75\n","19 + 85 = 104\n","1 + 12 = 13\n","21 + 54 = 75\n","84 + 73 = 157\n","31 + 98 = 129\n","76 + 57 = 133\n","17 + 28 = 45\n","79 + 39 = 118\n","97 + 64 = 161\n","84 + 20 = 104\n","60 + 53 = 113\n","11 + 79 = 90\n","78 + 59 = 137\n","69 + 16 = 85\n","78 + 83 = 161\n","60 + 79 = 139\n","20 + 54 = 74\n","13 + 44 = 57\n","83 + 4 = 87\n","57 + 37 = 94\n","34 + 65 = 99\n","13 + 19 = 32\n","41 + 19 = 60\n","65 + 28 = 93\n","9 + 64 = 73\n","18 + 52 = 70\n","17 + 74 = 91\n","8 + 83 = 91\n","51 + 2 = 53\n","54 +\n"]}]},{"cell_type":"code","source":["unique = sorted(set(text))\n","vocab_size = len(unique)\n","print(vocab_size)\n","print(unique)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBnXU-eyOwC4","executionInfo":{"status":"ok","timestamp":1738273514983,"user_tz":300,"elapsed":6,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"outputId":"4de432c9-e527-4e8c-e1eb-94cc195e5fda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["14\n","['\\n', ' ', '+', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=']\n"]}]},{"cell_type":"code","source":["# create a mapping from chars to ints\n","stoi = {ch:i for i, ch in enumerate(unique)}\n","itos = {i:ch for i, ch in enumerate(unique)}\n","encode = lambda s:[stoi[c] for c in s] # encoder: take a string, output a list of ints\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of ints, output a string\n","\n","print(encode(\"1 + 2 = 3\"))\n","print(decode(encode(\"1 + 2 = 3\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e4EbjUWPBjj","executionInfo":{"status":"ok","timestamp":1738273514983,"user_tz":300,"elapsed":4,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"outputId":"500c380d-4f86-436a-bf37-548b356077e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[4, 1, 2, 1, 5, 1, 13, 1, 6]\n","1 + 2 = 3\n"]}]},{"cell_type":"code","source":["data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.type)\n","print(data[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSoRPYmNPWpB","executionInfo":{"status":"ok","timestamp":1738273514983,"user_tz":300,"elapsed":4,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"outputId":"4a6096eb-9eec-4178-fd71-a7f291464adc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([100009]) <built-in method type of Tensor object at 0x7b70f17869f0>\n","tensor([11,  3,  1,  2,  1, 11,  3,  1, 13,  1,  4,  9,  3,  0, 11,  9,  1,  2,\n","         1, 12, 10,  1, 13,  1,  4, 11,  6,  0,  7,  6,  1,  2,  1,  4,  7,  1,\n","        13,  1,  8, 10,  0,  5,  3,  1,  2,  1,  6, 12,  1, 13,  1,  8, 12,  0,\n","         8,  7,  1,  2,  1,  9,  5,  1, 13,  1,  4,  4,  9,  0,  3,  1,  2,  1,\n","         6, 12,  1, 13,  1,  6, 12,  0,  8,  3,  1,  2,  1,  7, 12,  1, 13,  1,\n","        12, 12,  0,  8,  5,  1,  2,  1,  4,  9])\n"]}]},{"cell_type":"code","source":["# let's now split up the data into train and validation sets\n","n = int(0.9*len(data)) # first 90% will be train, rest is validation\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","\n","def get_batch(split):\n","    train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([torch.from_numpy(np.array(data[i:i+block_size], dtype=np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy(np.array(data[i+1:i+1+block_size], dtype=np.int64)) for i in ix])\n","\n","\n","    return x.to(device), y.to(device)"],"metadata":{"id":"yQPUNSWqZO2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import inspect\n","from dataclasses import dataclass\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","class LayerNorm(nn.Module):\n","    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n","\n","    def __init__(self, ndim, bias):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n","\n","class CausalSelfAttention(nn.Module):\n","\n","    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n","        super().__init__()\n","        # Á°Æ‰øù n_embd ÂèØ‰ª•Ë¢´ n_head Êï¥Èô§Ôºå ÊØè‰∏™Â§¥Áª¥Â∫¶Áõ∏Á≠â\n","        assert n_embd % n_head == 0\n","        # key, query, value projections for all heads, but in a batch\n","        # ‰∏Ä‰∏™linearÂ±ÇÔºåÂêåÊó∂ËÆ°ÁÆóQ, K, V; ËæìÂá∫Áª¥Â∫¶ÊòØÔºàB, T, 3 * n_embdÔºâ\n","        # ËæìÂÖ•ÁöÑxÊòØ(B, T, n_embd)\n","        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n","        # output projection\n","        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n","        # regularization\n","        self.attn_dropout = nn.Dropout(dropout)\n","        self.resid_dropout = nn.Dropout(dropout)\n","        self.n_head = n_head\n","        self.n_embd = n_embd\n","        self.dropout = dropout\n","        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        if not self.flash:\n","            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n","            # causal mask to ensure that attention is only applied to the left in the input sequence\n","            self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n","                                        .view(1, 1, block_size, block_size))\n","\n","    def forward(self, x):\n","        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n","\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        #tensor.split(a = ÊãÜÊàêÂ§öÂ§ß‰∏Ä‰∏™Ôºåb = Âú®Âì™‰∏™Áª¥Â∫¶‰∏äÊãÜ)\n","        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n","        # view(B, T, self.n_head, C // self.n_head) ÊãÜÂàÜÂ§öÂ§¥\n","        # n_embd Ë¢´ÊãÜÂàÜÊàê n_head ‰ªΩÔºåÊØè‰∏™Â§¥ÁöÑÁª¥Â∫¶ÊòØ C // n_head\n","        # (B, T, n_embd) ‚Üí (B, T, n_head, head_dim)\n","        # transpose(1, 2) ‰∫§Êç¢Áª¥Â∫¶\n","        # ‰∫§Êç¢ TÔºàÂ∫èÂàóÈïøÂ∫¶ÔºâÂíå n_head\n","        # (B, T, n_head, head_dim) -> (B, n_head, T, head_dim)\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n","\n","        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n","        if self.flash:\n","            # efficient attention using Flash Attention CUDA kernels\n","            # Ëøô‰∏™ÂáΩÊï∞Â∞±ÊòØÂÆûÁé∞Ê†áÂáÜÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ËøêÁÆó\n","            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n","        else:\n","            # manual implementation of attention\n","            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n","            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n","            att = F.softmax(att, dim=-1)\n","            att = self.attn_dropout(att)\n","            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n","        # transpose(1, 2)ÁöÑ‰ΩúÁî®Ôºö (B, n_head, T, head_dim) ‚Üí (B, T, n_head, head_dim)\n","        # contiguous() ËÆ©Âº†ÈáèÂú®ÂÜÖÂ≠ò‰∏≠ËøûÁª≠Â≠òÂÇ®Ôºå‰ª•‰æø view() Ê≠£Â∏∏ËøêË°å\n","        # view(B, T, C) ÂêàÂπ∂ n_head Âíå head_dimÔºåÂèòÂõû n_embd\n","        # (B, T, n_head, head_dim) ‚Üí (B, T, n_embd)\n","        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n","\n","        # output projection\n","        y = self.resid_dropout(self.c_proj(y))\n","        return y\n","\n","class MLP(nn.Module):\n","\n","    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n","        super().__init__()\n","        # ‰ΩúÁî®ÔºöÂ∞Ü ËæìÂÖ•ÁöÑ 768 Áª¥ÂêëÈáèÊâ©Â±ïÂà∞ 3072 Áª¥ÔºåÁõ∏ÂΩì‰∫éÂ¢ûÂä†‰∫ÜÁâπÂæÅÁª¥Â∫¶\n","        # ÁõÆÁöÑÔºöÊõ¥È´òÁª¥Â∫¶ÁöÑÁâπÂæÅÁ©∫Èó¥Ôºå‰ΩøÊ®°ÂûãÊõ¥ÂÆπÊòìÂ≠¶‰π†Â§çÊùÇÂÖ≥Á≥ª\n","        self.c_fc = nn.Linear(n_embd, 4 * n_embd, bias=bias)\n","        # ÊøÄÊ¥ªÂáΩÊï∞ GELUÔºàGaussian Error Linear UnitÔºâ\n","        # GELU ÊòØ ReLU ÁöÑÊîπËøõÁâàÔºåÂ∏∏Áî®‰∫é Transformer Ê®°Âûã\n","        self.gelu = nn.GELU()\n","        # ÂèòÂõû 768 Áª¥\n","        self.c_proj  = nn.Linear(4 * n_embd, n_embd, bias=bias)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.c_fc(x) # (B, T, 768) -> (B, T, 3072)\n","        x = self.gelu(x) # GELU ÊøÄÊ¥ªÂáΩÊï∞\n","        x = self.c_proj(x) # (B, T, 3072) -> (B, T, 768)\n","        x = self.dropout(x)\n","        return x\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n","        super().__init__()\n","        self.ln_1 = LayerNorm(n_embd, bias=bias)\n","        self.attn = CausalSelfAttention(n_embd, n_head, dropout, block_size, bias=True)\n","        self.ln_2 = LayerNorm(n_embd, bias=bias)\n","        self.mlp = MLP(n_embd, n_head, dropout, block_size, bias=True)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln_1(x))\n","        x = x + self.mlp(self.ln_2(x))\n","        return x\n","\n","\n","class GPT(nn.Module):\n","\n","    def __init__(self, vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=True):\n","        super().__init__()\n","        assert vocab_size is not None\n","        assert block_size is not None\n","        self.vocab_size = vocab_size\n","        self.block_size = block_size\n","        self.n_embd = n_embd\n","        self.n_layer = n_layer\n","        self.n_head = n_head\n","        self.dropout = dropout\n","        self.bias = bias\n","\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(vocab_size, n_embd), # token embeddings\n","            wpe = nn.Embedding(block_size, n_embd), # positional embeddings\n","            drop = nn.Dropout(dropout),\n","            h = nn.ModuleList([Block(n_embd, n_head, dropout, block_size, bias=bias) for _ in range(n_layer)]), # a stack of n_layer blocks\n","            ln_f = LayerNorm(n_embd, bias=bias), # final layer norm\n","        ))\n","        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False) # projects the final transformer output to the vocab size\n","\n","        # init all weights\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        device = idx.device\n","        b, t = idx.size()\n","        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.cblock_size}\"\n","        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n","\n","        # forward the GPT model itself\n","        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n","        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n","        x = self.transformer.drop(tok_emb + pos_emb)\n","        for block in self.transformer.h:\n","            x = block(x)\n","        x = self.transformer.ln_f(x)\n","\n","        logits = self.lm_head(x)  # Compute logits outside the if block\n","        loss = None\n","\n","        if targets is not None:\n","            # if we are given some desired targets also calculate the loss\n","            logits = self.lm_head(x)\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n","            # inference-time mini-optimization: only forward the lm_head on the very last position\n","            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n","\n","        return logits, loss\n","\n","    @torch.no_grad()\n","    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n","        \"\"\"\n","        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n","        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n","        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n","        \"\"\"\n","        for _ in range(max_new_tokens):\n","            # if the sequence context is growing too long we must crop it at block_size\n","            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n","            # forward the model to get the logits for the index in the sequence\n","            logits, _ = self(idx_cond)\n","            # pluck the logits at the final step and scale by desired temperature\n","            logits = logits[:, -1, :] / temperature\n","            # optionally crop the logits to only the top k options\n","            if top_k is not None:\n","                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","                logits[logits < v[:, [-1]]] = -float('Inf')\n","            # apply softmax to convert logits to (normalized) probabilities\n","            probs = F.softmax(logits, dim=-1)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","            # append sampled index to the running sequence and continue\n","            idx = torch.cat((idx, idx_next), dim=1)\n","\n","        return idx"],"metadata":{"id":"XVIg1aizaT7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_iters = 200\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"],"metadata":{"id":"cNC6DqTgcbZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32 # how many independent sequences will we process in parallel?\n","block_size = 256 # what is the maximum context length for predictions?\n","max_iters = 25\n","num_epochs = 150\n","eval_interval = 100\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 20\n","n_embd = 256\n","n_head = 8\n","n_layer = 6\n","dropout = 0.1\n","torch.manual_seed(1337)\n","bias = True # if using bias inside all Linear layers\n","block_size = 1024\n","vocab_size = len(unique)"],"metadata":{"id":"xOdwJ26Ax2e6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=bias)\n","m = model.to(device)"],"metadata":{"id":"DPl7iF7e2D-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"],"metadata":{"id":"snZoFt8qe76q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n","    # ÊØè 5 ‰∏™ epoch ËØÑ‰º∞‰∏ÄÊ¨°ÊçüÂ§±\n","    if epoch % 5 == 0:\n","        losses = estimate_loss()\n","        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    for iter in range(max_iters):\n","        # sample a batch of data\n","        xb, yb = get_batch('train')\n","\n","        # evaluate the loss\n","        logits, loss = model(xb, yb)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","\n","    scheduler.step()  # Ë∞ÉÊï¥Â≠¶‰π†Áéá\n"],"metadata":{"id":"odtqfBE0swtz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738273975296,"user_tz":300,"elapsed":460150,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"outputId":"252cc262-eec0-4204-b0a8-0e74e09edede"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5.008384 M parameters\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining Progress:   0%|          | 0/150 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["step 0: train loss 2.8314, val loss 2.8317\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:   3%|‚ñé         | 5/150 [00:15<07:01,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 5: train loss 1.8242, val loss 1.8241\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:   7%|‚ñã         | 10/150 [00:30<06:47,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 10: train loss 1.8141, val loss 1.8138\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  10%|‚ñà         | 15/150 [00:45<06:33,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 15: train loss 1.4973, val loss 1.4958\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  13%|‚ñà‚ñé        | 20/150 [01:01<06:18,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 20: train loss 1.1262, val loss 1.1259\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  17%|‚ñà‚ñã        | 25/150 [01:16<06:03,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 25: train loss 1.0981, val loss 1.0988\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  20%|‚ñà‚ñà        | 30/150 [01:31<05:49,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 30: train loss 1.0849, val loss 1.0848\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  23%|‚ñà‚ñà‚ñé       | 35/150 [01:47<05:34,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 35: train loss 1.0702, val loss 1.0687\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  27%|‚ñà‚ñà‚ñã       | 40/150 [02:02<05:20,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 40: train loss 1.0203, val loss 1.0207\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  30%|‚ñà‚ñà‚ñà       | 45/150 [02:17<05:05,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 45: train loss 0.9955, val loss 0.9960\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  33%|‚ñà‚ñà‚ñà‚ñé      | 50/150 [02:33<04:51,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 50: train loss 0.9766, val loss 0.9771\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  37%|‚ñà‚ñà‚ñà‚ñã      | 55/150 [02:48<04:36,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 55: train loss 0.8611, val loss 0.8611\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  40%|‚ñà‚ñà‚ñà‚ñà      | 60/150 [03:03<04:21,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 60: train loss 0.8387, val loss 0.8384\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 65/150 [03:19<04:07,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 65: train loss 0.8102, val loss 0.8095\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 70/150 [03:34<03:52,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 70: train loss 0.7683, val loss 0.7678\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 75/150 [03:49<03:38,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 75: train loss 0.7277, val loss 0.7270\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 80/150 [04:05<03:23,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 80: train loss 0.7064, val loss 0.7070\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 85/150 [04:20<03:09,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 85: train loss 0.7002, val loss 0.7001\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 90/150 [04:35<02:54,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 90: train loss 0.6960, val loss 0.6954\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 95/150 [04:51<02:40,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 95: train loss 0.6901, val loss 0.6904\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 100/150 [05:06<02:25,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 100: train loss 0.6837, val loss 0.6840\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 105/150 [05:21<02:11,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["step 105: train loss 0.6737, val loss 0.6731\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 110/150 [05:37<01:56,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["step 110: train loss 0.6552, val loss 0.6557\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 115/150 [05:52<01:42,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["step 115: train loss 0.6233, val loss 0.6243\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 120/150 [06:07<01:27,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["step 120: train loss 0.5729, val loss 0.5745\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 125/150 [06:23<01:12,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["step 125: train loss 0.5015, val loss 0.5026\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 130/150 [06:38<00:58,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 130: train loss 0.4078, val loss 0.4099\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 135/150 [06:54<00:43,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["step 135: train loss 0.3192, val loss 0.3187\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 140/150 [07:09<00:29,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 140: train loss 0.2425, val loss 0.2434\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 145/150 [07:24<00:14,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["step 145: train loss 0.1813, val loss 0.1837\n"]},{"output_type":"stream","name":"stderr","text":["Training Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [07:40<00:00,  3.07s/it]\n"]}]},{"cell_type":"code","source":["# early_stopping\n","# accuracy function"],"metadata":{"id":"sEKyA9IlOe5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(decode(model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=2000)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xRXry9lmqEN","executionInfo":{"status":"ok","timestamp":1738273981943,"user_tz":300,"elapsed":6650,"user":{"displayName":"ZIFEI BAI","userId":"03045138665814544162"}},"outputId":"c852df8f-9d43-4d3c-b212-a27df2d0990a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","20 + 10 = 30\n","32 + 47 = 79\n","13 + 80 = 93\n","50 + 0 = 60\n","63 + 54 = 117\n","44 + 76 = 120\n","77 + 64 = 141\n","8 + 59 = 67\n","1 + 33 = 34\n","81 + 97 = 178\n","3 + 53 = 86\n","93 + 64 = 157\n","93 + 86 = 179\n","80 + 97 = 179\n","12 + 7 = 27\n","16 + 0 = 17\n","79 + 13 = 102\n","99 + 49 = 148\n","83 + 57 = 140\n","73 + 37 = 110\n","37 + 69 = 106\n","25 + 71 = 96\n","66 + 60 = 126\n","16 + 96 = 111\n","54 + 30 = 10\n","29 + 13 = 42\n","5 + 69 = 74\n","8 + 2 = 9\n","2 + 16 = 17\n","64 + 15 = 89\n","30 + 99 = 127\n","18 + 74 = 92\n","42 + 95 = 137\n","87 + 38 = 125\n","15 + 67 = 82\n","39 + 29 = 68\n","84 + 69 = 153\n","71 + 38 = 108\n","31 + 10 = 41\n","95 + 64 = 159\n","69 + 69 = 138\n","81 + 96 = 177\n","12 + 83 = 95\n","54 + 50 = 104\n","61 + 57 = 118\n","90 + 86 = 176\n","43 + 52 = 95\n","17 + 59 = 76\n","12 + 4 = 26\n","76 + 57 = 133\n","87 + 86 = 173\n","14 + 17 = 31\n","13 + 34 = 46\n","45 + 3 = 48\n","7 + 48 = 55\n","56 + 85 = 117\n","43 + 29 = 72\n","51 + 29 = 80\n","7 + 66 = 73\n","80 + 57 = 137\n","90 + 48 = 128\n","63 + 37 = 100\n","57 + 39 = 96\n","65 + 89 = 153\n","65 + 19 = 82\n","93 + 8 = 99\n","14 + 34 = 48\n","32 + 19 = 51\n","33 + 30 = 63\n","41 + 38 = 79\n","41 + 22 = 63\n","98 + 16 = 114\n","56 + 53 = 109\n","4 + 37 = 109\n","65 + 61 = 126\n","85 + 16 = 101\n","72 + 12 = 84\n","34 + 18 = 52\n","0 + 90 = 91\n","73 + 89 = 162\n","61 + 83 = 144\n","36 + 36 = 72\n","85 + 91 = 176\n","29 + 4 = 33\n","6= + 75 = 140\n","76 + 13 = 89\n","45 + 22 = 67\n","15 + 22 = 37\n","31 + 38 = 59\n","77 + 62 = 139\n","55 + 38 = 93\n","24 + 96 = 120\n","79 + 85 = 174\n","18 + 7 = 24\n","65 + 17 = 72\n","61 + 28 = 89\n","17 + 62 = 79\n","89 + 68 = 157\n","38 + 34 = 72\n","35 + 90 = 125\n","57 + 70 = 127\n","27 + 78 = 105\n","53 + 37 = 90\n","30 + 9 = 39\n","67 + 32 = 99\n","91 + 61 = 156\n","79 + 35 = 113\n","28 + 20 = 48\n","72 + 8 = 76\n","1 + 63 = 64\n","63 + 78 = 140\n","29 + 15 = 44\n","74 + 25 = 99\n","45 + 44 = 89\n","79 + 20 = 99\n","20 + 56 = 76\n","71 + 22 = 93\n","44 + 85 = 129\n","74 + 22 = 96\n","11 + 47 = 58\n","14 + 40 = 54\n","99 + 12 = 111\n","48 + 30 = 78\n","27 + 11 = 38\n","50 + 37 = 87\n","59 + 22 = 81\n","11 + 42 = 53\n","94 + 42 = 136\n","65 + 74 = 149\n","97 + 7 = 105\n","12 + 45 = 57\n","32 + 3 = 35\n","25 + 4 = 29\n","18 + 91 = 104\n","40 + 20 = 60\n","71 + 66 = 137\n","0 + 76 = 76\n","82 + 85 = 167\n","84 + 95 = 179\n","37 + 46 = 83\n","40 + 67 = 109\n","23 + 87 = 110\n","11 + 71 = 82\n","6 + 60 = 66\n","49 + 78 = 128\n","6 + 81 = 87\n","80 + 80 = 160\n","52 + 58 = 110\n","80 + 23 = 103\n","3 + 41 = 44\n","53 + 44 = 97\n","3\n"]}]}]}