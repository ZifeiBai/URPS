{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZifeiBai/URPS/blob/main/str_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhYBYz_Yj1yq",
        "outputId": "7336001b-a696-4a01-9cdb-a7a8536d79aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "import math\n",
        "import random\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "import wandb"
      ],
      "metadata": {
        "id": "y2jemn0ZkDm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '&', '*']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "padding_token_index = 12\n",
        "end_token_index = 11\n",
        "\n",
        "# create a mapping from chars to ints\n",
        "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
        "itos = {i:ch for i, ch in enumerate(vocab)}\n",
        "encode = lambda s:[stoi[c] for c in s] # encoder: take a string, output a list of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of ints, output a string\n",
        "\n",
        "print(encode(\"12=3&\"))\n",
        "print(decode(encode(\"12=3&\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIM_06ltkW26",
        "outputId": "ebb99096-051c-4d8c-8db2-f128376a9135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 10, 3, 11]\n",
            "12=3&\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1000 # how many independent sequences will we process in parallel?\n",
        "block_size = 60 # what is the maximum context length for predictions?\n",
        "max_iters = 5000 # CHANGE the step size\n",
        "eval_interval = 100\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "bias = True\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "sUKn-XQwwfzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias=False): # class constructor\n",
        "        super().__init__()\n",
        "        # nn.Parameter, pytorch optimize will update the value of this parameter during training\n",
        "        self.weight = nn.Parameter(torch.ones(ndim)) # trainable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None # trainable parameter\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-6)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
        "\n",
        "        # Store hyperparameters\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Key, Query, Value projections\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
        "\n",
        "        # T-5 PE\n",
        "        # self.rel_pos_bias = T5RelativePositionBias(block_size, n_head)\n",
        "\n",
        "        # Regularization\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "                # Check for Flash Attention availability\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # Causal mask for slow attention\n",
        "            self.register_buffer(\n",
        "                \"bias\",\n",
        "                torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # Batch size, sequence length, embedding dimension\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)  # Split into Q, K, V (B, T, n_embd)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "\n",
        "        # Compute T5 relative position bias\n",
        "        # self.rel_pos_bias = self.rel_pos_bias.to(device)  # Move to correct device\n",
        "        # rel_bias = self.rel_pos_bias(T, device)  # Compute relative position bias\n",
        "        # (1, num_heads, T, T)\n",
        "\n",
        "        # Flash Attention or fallback to manual implementation\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q, k, v,\n",
        "                attn_mask=None,\n",
        "                dropout_p=self.dropout if self.training else 0,\n",
        "                is_causal=True\n",
        "            )\n",
        "        # else:\n",
        "        # Manual attention with causal masking\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))  # Scaled dot product\n",
        "        # # att = att + rel_bias  # Apply relative positional bias\n",
        "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))  # Apply causal mask\n",
        "        # att = F.softmax(att, dim=-1)  # Normalize attention scores\n",
        "        # att = self.attn_dropout(att)\n",
        "        # y = att @ v  # Apply attention weights to values (B, n_head, T, head_size)\n",
        "\n",
        "        # Reshape back to original format\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # Reassemble heads\n",
        "\n",
        "        # Output projection and residual dropout\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "# SwiGLU used in llama\n",
        "class SwiGLUFFN(nn.Module):\n",
        "    def __init__(self, n_embd: int, dropout: float = 0.0, bias: bool = False):\n",
        "        super().__init__()\n",
        "        d_ff = int((8/3) * n_embd)\n",
        "        self.fc1 = nn.Linear(n_embd, 2 * d_ff, bias=bias)\n",
        "        self.fc2 = nn.Linear(d_ff, n_embd, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x_proj = self.fc1(x)\n",
        "        x1, x2 = x_proj.chunk(2, dim=-1)\n",
        "        swish = x1 * torch.sigmoid(x1)\n",
        "        x = swish * x2\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n",
        "        super().__init__()\n",
        "        # LayerNorm and CausalSelfAttention with explicit parameters\n",
        "        self.ln_1 = LayerNorm(n_embd, bias=bias)\n",
        "        self.attn = CausalSelfAttention(n_embd, n_head, dropout, block_size, bias=bias)\n",
        "        self.ln_2 = LayerNorm(n_embd, bias=bias)\n",
        "        # self.mlp = MLP(n_embd, dropout, bias=bias)  # MLP with explicit parameters\n",
        "        self.mlp = SwiGLUFFN(n_embd, dropout) #bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply residual connection and pre-normalization\n",
        "        x = x + self.attn(self.ln_1(x))  # Apply LayerNorm before attention\n",
        "        x = x + self.mlp(self.ln_2(x))  # Apply LayerNorm before MLP\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=True):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        super().__init__()\n",
        "        assert vocab_size is not None\n",
        "        assert block_size is not None\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.dropout = dropout\n",
        "        self.bias = bias\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(vocab_size, n_embd), # token embeddings\n",
        "            # wpe = nn.Embedding(block_size, n_embd), # positional embeddings CHANGE, t-5 positional embedding\n",
        "            drop = nn.Dropout(dropout),\n",
        "            h = nn.ModuleList([Block(n_embd, n_head, dropout, block_size, bias=bias) for _ in range(n_layer)]), # a stack of n_layer blocks\n",
        "            ln_f = LayerNorm(n_embd, bias=bias), # final layer norm\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False) # projects the final transformer output to the vocab size\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        # pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        # pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb)# + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=encode(\"*\")[0])\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            # loss = None\n",
        "\n",
        "        return logits, loss"
      ],
      "metadata": {
        "id": "UscFoJ1JkdRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, idx, max_new_tokens, temperature=0.00001, top_k=None):\n",
        "    \"\"\"\n",
        "    Generate a sequence of tokens given an initial sequence.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The model used for generation.\n",
        "        idx (torch.Tensor or list): Initial sequence of indices (LongTensor of shape (b,t)).\n",
        "        max_new_tokens (int): Number of new tokens to generate.\n",
        "        temperature (float): Scaling factor for logits before softmax.\n",
        "        top_k (int, optional): If specified, restricts sampling to top k tokens.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The generated sequence.\n",
        "    \"\"\"\n",
        "    #idx = idx.unsqueeze(0) if idx.dim() == 1 else idx\n",
        "    #idx = torch.tensor(idx, device=model.device) if not isinstance(idx, torch.Tensor) else idx.to(model.device)\n",
        "    batch_size, seq_len = idx.shape\n",
        "    idx = idx.to(model.device)\n",
        "\n",
        "    # Track which sequences are still active (not finished)\n",
        "    is_active = torch.ones(batch_size, dtype=torch.bool, device=model.device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if not is_active.any():\n",
        "            break\n",
        "        # Ensure context length does not exceed model's block size\n",
        "        idx_cond = idx if idx.size(1) <= model.block_size else idx[:, -model.block_size:]\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits, _ = model(idx_cond)\n",
        "\n",
        "        # Extract logits for the last token and apply temperature scaling\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Apply top-k filtering if necessary\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)), dim=-1)\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sample next token\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            if is_active[i] and idx_next[i].item() == encode('&')[0]:\n",
        "                is_active[i] = False  # if \"&\" appears, stop generating\n",
        "\n",
        "        # Stop if all sequences have reached `end_token_index`\n",
        "        if not is_active.any():\n",
        "            break\n",
        "\n",
        "        # Append sampled token to sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    decoded_texts = []\n",
        "    for seq in idx.tolist():\n",
        "        text = decode(seq)\n",
        "        cut_text = text.split('&')[0]  # make sure generate tokens don't have \"&\", only got tokens before \"&\"\n",
        "        decoded_texts.append(cut_text)\n",
        "\n",
        "    return decoded_texts"
      ],
      "metadata": {
        "id": "Zx887ztRk-9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_origin_dataset(original, task, num_samples = 2000000):\n",
        "    file_path = f\"/content/drive/MyDrive/URPS/Data/origin_ds_{task}.txt\"\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"File {file_path} already exists.\\nSkipping generation.\")\n",
        "        return\n",
        "    if task == 'copy':\n",
        "        # generate 200000 sample\n",
        "        a_values = np.random.randint(1, original + 1, size=num_samples)\n",
        "        strings = [\"\".join(np.random.choice([str(i) for i in range(10)], size=a)) for a in a_values]  # random generate strings\n",
        "        target = strings\n",
        "        to_write = [f\"{a}={b}&\" for a, b in zip(strings, target)]\n",
        "\n",
        "        # write down\n",
        "        with open(file_path, \"w\") as f:\n",
        "            f.write(\"\\n\".join(to_write))\n",
        "\n",
        "    print(f\"{num_samples} original data for task {task} is saved in {file_path}\")"
      ],
      "metadata": {
        "id": "NFz93vUdlEy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create 50000 OOD data, save\n",
        "def generate_prompt_OOD(si_round, task, original):\n",
        "    \"\"\"\n",
        "    Return a list of 'num_prompts' strings for task\n",
        "    with 'original+si_round' digits each.\n",
        "    \"\"\"\n",
        "    if task == 'copy':\n",
        "        strings = \"\".join(np.random.choice([str(i) for i in range(10)], size=si_round+original))\n",
        "        prompt_str = f\"{str(strings)}=\"  # e.g. '1235455='\n",
        "\n",
        "    return prompt_str\n",
        "\n",
        "\n",
        "def gen_si_data(model, si_round, task, num_samples=100000, block_size=block_size, batch_size=batch_size):\n",
        "    output_path = f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_round-1}.txt\"\n",
        "    num_batches = (num_samples) // batch_size + 1\n",
        "    print(f\"Generating {si_round} si data...\")\n",
        "    for _ in range(num_batches):\n",
        "        # generate 'batch_size' prompts of digit length (original + si_round)\n",
        "        prompts = [generate_prompt_OOD(si_round, task, original=10) for _ in range(batch_size)]\n",
        "        encoded_prompts = []\n",
        "\n",
        "        for prompt_str in prompts: # iterate through all 1000 prompts\n",
        "            # encode and convert prompt_str into tensor\n",
        "            prompt_ids = encode(prompt_str)\n",
        "            encoded_prompts.append(prompt_ids)  # Add encoded prompt to the list\n",
        "\n",
        "        prompt_tensor = torch.tensor(encoded_prompts, dtype=torch.long, device=device)\n",
        "        out_str = generate(\n",
        "            model=model,\n",
        "            idx=prompt_tensor,\n",
        "            max_new_tokens=35,\n",
        "            top_k=1\n",
        "        )\n",
        "\n",
        "        # length filter\n",
        "        out_str = [text for text in out_str if len(text[(si_round+11):]) == (si_round + 10)]\n",
        "\n",
        "        # print(len(out_str[0]))\n",
        "        # print(out_str)\n",
        "        # check number of lines in this file\n",
        "        if os.path.exists(output_path):\n",
        "            with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                current_lines = sum(1 for _ in f)\n",
        "        else:\n",
        "            current_lines = 0\n",
        "\n",
        "        # If we already have 50,000 lines, stop\n",
        "        if current_lines >= 50000:\n",
        "            print(f\"Already reached 50,000 lines. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        # calculate remaining lines\n",
        "        remaining = max(0, 50000 - current_lines)  # Prevent negative values\n",
        "        to_write = out_str[:remaining]  # Only write needed amount\n",
        "\n",
        "\n",
        "        # append write down\n",
        "        with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.writelines([line + \"&\\n\" for line in to_write])\n",
        "\n",
        "        # if 50000 rows, break\n",
        "        # if len(to_write) < batch_size:\n",
        "        #     break\n",
        "\n",
        "    print(f\"Writing complete. \")"
      ],
      "metadata": {
        "id": "qyYepnmulMwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, batch_size=batch_size, block_size=block_size):\n",
        "    \"\"\"data is combined dataset, get combined dataset in train loop\"\"\"\n",
        "    final_sample = random.sample(data, batch_size)\n",
        "    final_sample = [line.strip() for line in final_sample]\n",
        "\n",
        "    x_list, y_list = [], []\n",
        "    for x_str in final_sample:\n",
        "        # print(x_str)\n",
        "        x_encoded = encode(x_str)\n",
        "        x_padded = x_encoded + [padding_token_index] * (block_size - len(x_encoded))\n",
        "        x_list.append(torch.tensor(x_padded, dtype=torch.int64))\n",
        "        y_encoded = encode(x_str)[1:]\n",
        "        y_encoded.append(end_token_index)\n",
        "        y_padded = y_encoded + [padding_token_index] * (block_size - len(y_encoded))\n",
        "        y_list.append(torch.tensor(y_padded, dtype=torch.int64))\n",
        "\n",
        "    x_tensor = torch.stack(x_list).to(device)\n",
        "    y_tensor = torch.stack(y_list).to(device)\n",
        "    return x_tensor, y_tensor"
      ],
      "metadata": {
        "id": "C7vu2D-7lVCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/URPS/Data/origin_ds_copy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.readlines()"
      ],
      "metadata": {
        "id": "7PQla4tnS7Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_batch(data)[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdzGXrYaS418",
        "outputId": "7e8cb290-7e0e-41a2-8c73-c72bb7a24d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 60])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 100\n",
        "@torch.no_grad()\n",
        "def estimate_loss(data, model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        X, Y = get_batch(data)\n",
        "        padding_mask_x = (X != padding_token_index).long()\n",
        "        logits, loss = model(X, Y)\n",
        "        losses[k] = loss.item()\n",
        "    out['loss'] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "VJPFkJQIklWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for multiple training models for 90%+ accuracy\n",
        "def create_optimizer_and_scheduler(model, total, warm, decay):\n",
        "    # AdamW\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,              # learning rate\n",
        "        betas=(0.9, 0.99),\n",
        "        eps=1e-12,\n",
        "        weight_decay=0.1\n",
        "    )\n",
        "\n",
        "    # LR Scheduler\n",
        "    total_steps = total # CHANGE, CHECK max_iter\n",
        "    warmup_steps = warm\n",
        "    decay_steps = decay\n",
        "    stable_steps = total_steps - warmup_steps - decay_steps\n",
        "\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return step / warmup_steps  # Linear warmup 0->1\n",
        "        elif step < warmup_steps + stable_steps:\n",
        "            return 1.0                  # Stable\n",
        "        else:\n",
        "            # Cosine decay from 1->0\n",
        "            decay_ratio = (step - warmup_steps - stable_steps) / decay_steps\n",
        "            return 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "    return optimizer, scheduler"
      ],
      "metadata": {
        "id": "3G1kmYoelY9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for accuracy printing for each model\n",
        "def accuracy_print_one(model, num_digits, need_print=False):\n",
        "    correct = 0\n",
        "    total = 1000\n",
        "    num_batches = total // batch_size\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        prompts = [\"\".join(np.random.choice([str(i) for i in range(10)], size=num_digits)) + \"=\" for _ in range(batch_size)]  # random generate strings\n",
        "\n",
        "        context = torch.tensor([encode(inp) for inp in prompts], dtype=torch.long, device=device)\n",
        "\n",
        "        # output in batch\n",
        "        output_batch = generate(model=model, idx=context, max_new_tokens=35, top_k=1)\n",
        "\n",
        "        targets = [p + p[:-1] for p in prompts]\n",
        "        correct += sum([output == target for output, target in zip(output_batch, targets)])\n",
        "\n",
        "        # if needed, print wrong answer\n",
        "        if need_print:\n",
        "            for inp, out, target in zip(prompts, output_batch, targets):\n",
        "                if out != target:\n",
        "                    print(f\"   Input: {inp}\")\n",
        "                    print(f\"  Output: {out}\")\n",
        "                    print(f\"Expected: {target}\")\n",
        "                    print(\"-----------\")\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Accuracy for {num_digits} digits: {acc}\")\n",
        "    return acc\n",
        "\n",
        "\n",
        "def get_avg_performance(model, num_digits):\n",
        "    '''\n",
        "    Call this function for get the accuracy for each model\n",
        "    '''\n",
        "    dict_acc = {}\n",
        "    for num_dig in range(1, num_digits+1):\n",
        "        dict_acc[num_dig] = accuracy_print_one(model, num_dig, need_print=False)\n",
        "    return dict_acc\n",
        "\n",
        "def test_accuracy_on_digits(model, digits):\n",
        "    acc_list = []\n",
        "    for i in range(10):\n",
        "        acc_list.append(accuracy_print_one(model, digits, need_print=False))\n",
        "    return sum(acc_list)/len(acc_list)"
      ],
      "metadata": {
        "id": "-sT_PR1oD0fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed(seed)\n",
        "      torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "ij56xIdDLqt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "d1IuDEk1G2Jf",
        "outputId": "dc54fe55-bd6c-447c-e323-e05eb86bb8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbirdyyybai\u001b[0m (\u001b[33mbirdyyybai-university-of-michigan\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"transformer_si_graphs\",\n",
        "           config={\n",
        "            \"learning_rate\": 5e-4,\n",
        "            \"batch_size\": 1024,\n",
        "            \"block_size\": 35,\n",
        "            \"optimizer\": \"AdamW\",\n",
        "            \"n_embd\": 384,\n",
        "            \"n_head\": 6,\n",
        "            \"n_layer\": 6,\n",
        "            \"dropout\": 0.0,\n",
        "            \"max_iter\": 10000\n",
        "            },\n",
        "           name= \"si for 10\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "uruNl09gHAQY",
        "outputId": "0d27e39a-6d0b-465e-c0fd-c67b142583be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250311_021422-z567cos0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/birdyyybai-university-of-michigan/self-improve-transformer%201-10%20str_copy/runs/z567cos0' target=\"_blank\">train on 1-10, str-copying, pre</a></strong> to <a href='https://wandb.ai/birdyyybai-university-of-michigan/self-improve-transformer%201-10%20str_copy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/birdyyybai-university-of-michigan/self-improve-transformer%201-10%20str_copy' target=\"_blank\">https://wandb.ai/birdyyybai-university-of-michigan/self-improve-transformer%201-10%20str_copy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/birdyyybai-university-of-michigan/self-improve-transformer%201-10%20str_copy/runs/z567cos0' target=\"_blank\">https://wandb.ai/birdyyybai-university-of-michigan/self-improve-transformer%201-10%20str_copy/runs/z567cos0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/birdyyybai-university-of-michigan/self-improve-transformer%201-10%20str_copy/runs/z567cos0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7adca8d5be90>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_origin_dataset(original=10, task='copy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXOIHZeuGIrk",
        "outputId": "5fcf0261-5a45-4070-ff5e-078813c618f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File /content/drive/MyDrive/URPS/Data/origin_ds_copy.txt already exists.\n",
            "Skipping generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a base training loop for producing base model\n",
        "print(f\"Start run pretrain train loop with 5000 steps and 500 warm, 1000 decay\")\n",
        "data = []\n",
        "# INITIALIZE MODEL, OPTIMIZER, SHCEDULER\n",
        "model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=bias)\n",
        "m = model.to(device)\n",
        "with open(\"/content/drive/MyDrive/URPS/Data/origin_ds_copy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.readlines()\n",
        "optimizer, scheduler = create_optimizer_and_scheduler(model, 5000, 500, 1000)\n",
        "\n",
        "# TRAINNG LOOP:\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "loss_list = []\n",
        "\n",
        "scaler = GradScaler('cuda')\n",
        "for iter in tqdm(range(5000), desc=\"Training Progress\"):\n",
        "    # sample a batch of data\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses1 = estimate_loss(data, model)['loss']\n",
        "        print(f\"step {iter}: loss {losses1:.4f}\")\n",
        "        log_dict = {\"Loss\": losses1}\n",
        "        loss_list.append(round(losses1.item(), 4))\n",
        "        wandb.log(log_dict)\n",
        "\n",
        "    xb, yb = get_batch(data)\n",
        "\n",
        "    # evaluate the loss\n",
        "    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "        logits1, loss1 = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    scaler.scale(loss1).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"Training finished for pretrain.\\nEvaluating 11-digit accuracy...\")\n",
        "\n",
        "# evaluate final performance on digit addition\n",
        "acc = test_accuracy_on_digits(model, 11)\n",
        "print(f\"Average accuracy: {acc}\")\n",
        "filename = f\"base_model_str_copy.pt\"\n",
        "save_path = f\"/content/drive/MyDrive/URPS/Models/{filename}\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Saved best model at {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "x-b60pjqlaw-",
        "outputId": "0a0e1951-1394-457f-9c83-ca68198197e6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start run pretrain train loop with 5000 steps and 500 warm, 1000 decay\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 0/5000 [00:05<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 2.6501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "You must call wandb.init() before wandb.log()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-fb42991a8769>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlog_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlosses1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode(\"*\")[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vog0sy-ega1",
        "outputId": "1d50c306-8aca-4f35-fdba-700b02fb82b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()"
      ],
      "metadata": {
        "id": "rTQCgwRNLuO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model0 = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "model0.to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/URPS/Models/sc_model_0.pt\"\n",
        "model0.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model0.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHfSRjadUQr6",
        "outputId": "5801a7e6-8922-4366-c09a-87a8cec22460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-356630e38d9a>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model0.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(13, 384)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=384, out_features=1152, bias=True)\n",
              "          (c_proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): SwiGLUFFN(\n",
              "          (fc1): Linear(in_features=384, out_features=2048, bias=False)\n",
              "          (fc2): Linear(in_features=1024, out_features=384, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=384, out_features=13, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy_on_digits(model0, 11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYEaLOe0UVE9",
        "outputId": "b12a4a55-5870-4e75-cfdd-99fc5dd1e78c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 11 digits: 0.995\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.996\n",
            "Accuracy for 11 digits: 0.998\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.998\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.998\n",
            "Accuracy for 11 digits: 0.997\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9981"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_print_one(model0, 11, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niEgc-5l_Bk0",
        "outputId": "d49ff179-e8df-4430-b8e4-d961d14c1ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Input: 60472113615=\n",
            "  Output: 60472113615=6047211361\n",
            "Expected: 60472113615=60472113615\n",
            "-----------\n",
            "   Input: 28470563655=\n",
            "  Output: 28470563655=2847056365\n",
            "Expected: 28470563655=28470563655\n",
            "-----------\n",
            "Accuracy for 11 digits: 0.998\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.998"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_dict = {}\n",
        "# forgot to add in acc_dict for these two trains, so I added them manually.\n",
        "acc_dict[\"1000\"] = [0.9993, 0.9967, 0.9939, 0.9892, 0.9771, 0.9621, 0.9501, 0.921, 0.891, 0.8483, 0.796]\n",
        "acc_dict[\"500\"] = [0.9993, 0.9985, 0.9941, 0.9911, 0.98, 0.9579, 0.9306, 0.8874, 0.8262, 0.7423, 0.6345]"
      ],
      "metadata": {
        "id": "nKVjAh3BZ0Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"transformer_si_graphs\",\n",
        "           config={\n",
        "            \"learning_rate\": 5e-4,\n",
        "            \"batch_size\": 1024,\n",
        "            \"block_size\": 35,\n",
        "            \"optimizer\": \"AdamW\",\n",
        "            \"n_embd\": 384,\n",
        "            \"n_head\": 6,\n",
        "            \"n_layer\": 6,\n",
        "            \"dropout\": 0.0,\n",
        "            \"si_iter\": 1500,\n",
        "            \"decay\": 500\n",
        "            },\n",
        "           name= \"si for 10 rounds with length filter\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "FBDTLrFbXSLg",
        "outputId": "9909aeb7-5e54-459f-b0a2-f2106f4219bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250311_203102-rrp5vvvz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/rrp5vvvz' target=\"_blank\">si for 10 rounds with length filter</a></strong> to <a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs' target=\"_blank\">https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/rrp5vvvz' target=\"_blank\">https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/rrp5vvvz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/rrp5vvvz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7de0419ace10>"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "hjyfNVr7cWsd",
        "outputId": "6115d84f-d45f-4aaf-a4f9-7f980c9f326b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "updating run config (1.0s)<br>  <strong style=\"color:red\">ERROR</strong> retrying HTTP 409: run z6vsr9k4 was previously created and deleted; try a new run name"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-145-a2bce46dc485>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   4128\u001b[0m     \"\"\"\n\u001b[1;32m   4129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4130\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDummy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_attaching\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2104\u001b[0m                 ),\n\u001b[1;32m   2105\u001b[0m             )\n\u001b[0;32m-> 2106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m     def _finish(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_finish\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atexit_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m             \u001b[0;31m# Run hooks that should happen after the last messages to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2352\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_on_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2607\u001b[0m         ) as progress_printer:\n\u001b[1;32m   2608\u001b[0m             \u001b[0;31m# Wait for the run to complete.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2609\u001b[0;31m             wait_with_progress(\n\u001b[0m\u001b[1;32m   2610\u001b[0m                 \u001b[0mexit_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2611\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/wait_with_progress.py\u001b[0m in \u001b[0;36mwait_with_progress\u001b[0;34m(handle, timeout, progress_after, display_progress)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mEquivalent\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpassing\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mwait_all_with_progress\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \"\"\"\n\u001b[0;32m---> 24\u001b[0;31m     return wait_all_with_progress(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/mailbox/wait_with_progress.py\u001b[0m in \u001b[0;36mwait_all_with_progress\u001b[0;34m(handle_list, timeout, progress_after, display_progress)\u001b[0m\n\u001b[1;32m     85\u001b[0m             )\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0masyncio_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_loop_with_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/asyncio_compat.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.define_metric(\"train_loss\", step_metric=\"train_step\")  # Loss 走 train_step\n",
        "wandb.define_metric(\"Accuracy\", step_metric=\"digit_step\")  # Accuracy 走 digit_step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE2Kf-kki6Sx",
        "outputId": "b2d01853-5124-4574-a41f-7af4798a34cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_metric.Metric at 0x7de10f6c7fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is self-improve training process\n",
        "# model accuracy for 11 digits\n",
        "model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "model.to(device)\n",
        "checkpoint_path = f\"/content/drive/MyDrive/URPS/Models/sc_model_{0}.pt\"\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "digit_step = 11\n",
        "train_step = 0\n",
        "acc = test_accuracy_on_digits(model, 11)\n",
        "wandb.log({\"Accuracy\": acc, \"digit_step\": digit_step})\n",
        "\n",
        "for si_r in range(1, 11):\n",
        "    # first get last round model, generate self-improve data\n",
        "    model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "    model.to(device)\n",
        "    checkpoint_path = f\"/content/drive/MyDrive/URPS/Models/sc_model_{si_r-1}.pt\"\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    gen_si_data(model, si_r, 'copy')\n",
        "\n",
        "    # get combined data\n",
        "    data = []\n",
        "    if si_r == 1:\n",
        "        # for first si round, we need the original dataset\n",
        "        with open(\"/content/drive/MyDrive/URPS/Data/origin_ds_copy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data = f.readlines()\n",
        "        # get the first si data, combine them together\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_r-1}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            # because 50% data from before, 50% from latest si data, copy (39+si_r) times\n",
        "            # Let both datasets have the same amount of data and then randomly draw from the combined dataset\n",
        "            sub_data = f.readlines()\n",
        "            wrong = 0\n",
        "            for i in range(len(sub_data)):\n",
        "                if sub_data[i][:(si_r+10)] != sub_data[i][(si_r+10+1): (si_r+10+1+si_r+10)]:\n",
        "                    wrong +=1\n",
        "            print(f\"This filtered file has {(wrong / len(sub_data))*100}% wrong answer. \")\n",
        "            data += sub_data * (39+si_r)\n",
        "    else:\n",
        "        # for subsequent si round, we need all data from before\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/{si_r-1}_round_combined_ds.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data = f.readlines()\n",
        "        # get new si data, combine them together\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_r-1}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            # because 50% data from before, 50% from latest si data, copy (39+si_r) times\n",
        "            # Let both datasets have the same amount of data and then randomly draw from the combined dataset\n",
        "            sub_data = f.readlines()\n",
        "            wrong = 0\n",
        "            for i in range(len(sub_data)):\n",
        "                if sub_data[i][:(si_r+10)] != sub_data[i][(si_r+10+1): (si_r+10+1+si_r+10)]:\n",
        "                    wrong +=1\n",
        "            print(f\"This filtered file has {(wrong / len(sub_data))*100}% wrong answer. \")\n",
        "            data += sub_data * (39+si_r)\n",
        "    random.shuffle(data)\n",
        "    print(f\"This is round {si_r}, The data used for training has {len(data)/1e6} M rows\")\n",
        "\n",
        "    optimizer, scheduler = create_optimizer_and_scheduler(model, wandb.config[\"si_iter\"], 0, wandb.config[\"decay\"])\n",
        "    m = model.to(device)\n",
        "    # TRAINNG LOOP:\n",
        "    # print the number of parameters in the model\n",
        "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "    loss_list = []\n",
        "\n",
        "    scaler = GradScaler('cuda')\n",
        "    for iter in tqdm(range(wandb.config[\"si_iter\"]), desc=\"Training Progress\"):\n",
        "        # sample a batch of data\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses = estimate_loss(data, model)['loss']\n",
        "            print(f\"step {iter}: loss {losses:.4f}\")\n",
        "            loss_list.append(round(losses.item(), 4))\n",
        "            wandb.log({\"train_loss\": losses.item(), \"train_step\": train_step})\n",
        "            train_step += 1\n",
        "\n",
        "        xb, yb = get_batch(data)\n",
        "\n",
        "        # evaluate the loss\n",
        "        with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "            logits1, loss1 = model(xb, yb)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        scaler.scale(loss1).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"Training finished for self-improve round {si_r}.\\nEvaluating {10+si_r+1}-digit accuracy...\")\n",
        "\n",
        "    # evaluate final performance on digit addition\n",
        "    acc = test_accuracy_on_digits(model, 10+si_r+1)\n",
        "    digit_step = 10+si_r+1\n",
        "    wandb.log({\"Accuracy\": acc, \"digit_step\": digit_step})\n",
        "\n",
        "    print(f\"Average accuracy for {10+si_r+1}: {acc}\")\n",
        "    filename = f\"sc_model_{si_r}.pt\"\n",
        "    save_path = f\"/content/drive/MyDrive/URPS/Models/{filename}\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Saved best model at {save_path}\")\n",
        "\n",
        "\n",
        "    data_smaller, data_larger = [], []\n",
        "    if si_r == 1:\n",
        "        # get original data\n",
        "        with open(\"/content/drive/MyDrive/URPS/Data/origin_ds_copy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data_larger = f.readlines()\n",
        "        # get si data\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_r-1}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data_smaller = f.readlines()\n",
        "    else:\n",
        "        # get all data before\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/{si_r-1}_round_combined_ds.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data_larger = f.readlines()\n",
        "        # get si data\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_r-1}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data_smaller = f.readlines()\n",
        "    print(f\"This is round {si_r}, data larger has {len(data_larger)} rows\")\n",
        "    print(f\"This is round {si_r}, data smaller has {len(data_smaller)} rows\")\n",
        "\n",
        "    # combine this two dataset as one new combined dataset\n",
        "    data_new = data_larger + data_smaller\n",
        "    random.shuffle(data_new)\n",
        "\n",
        "    with open(f\"/content/drive/MyDrive/URPS/Data/{si_r}_round_combined_ds.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines([line if line.endswith(\"\\n\") else line + \"\\n\" for line in data_new])\n",
        "\n",
        "    print(f\"{si_r}_round_combined_ds.txt has {len(data_new)} rows\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c_BHAlr_ZLwi",
        "outputId": "9eb58962-a4be-410b-ecac-869bf0c58408",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-148-5ed5ba2f2407>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 11 digits: 0.997\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.997\n",
            "Accuracy for 11 digits: 0.997\n",
            "Accuracy for 11 digits: 0.998\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-148-5ed5ba2f2407>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 1 si data...\n",
            "Already reached 50,000 lines. Stopping early.\n",
            "Writing complete. \n",
            "This filtered file has 0.008% wrong answer. \n",
            "This is round 1, The data used for training has 4.0 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:05<2:28:50,  5.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.2162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:21<41:47,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:38<36:46,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:54<36:22,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:10<32:50,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:26<26:33,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:42<26:41,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [01:59<16:56,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:15<16:52,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:31<16:10,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:47<14:52,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:04<09:51,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1202/1500 [03:20<06:18,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:36<05:56,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:53<02:57,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:03<00:00,  6.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for self-improve round 1.\n",
            "Evaluating 12-digit accuracy...\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.997\n",
            "Accuracy for 12 digits: 0.997\n",
            "Accuracy for 12 digits: 0.996\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 0.997\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 0.996\n",
            "Average accuracy for 12: 0.9974999999999999\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_1.pt\n",
            "This is round 1, data larger has 2000000 rows\n",
            "This is round 1, data smaller has 50000 rows\n",
            "1_round_combined_ds.txt has 2050000 rows\n",
            "Generating 2 si data...\n",
            "Already reached 50,000 lines. Stopping early.\n",
            "Writing complete. \n",
            "This filtered file has 0.02% wrong answer. \n",
            "This is round 2, The data used for training has 4.1 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:30:00,  6.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.2049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:21<33:28,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:38<39:25,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<35:56,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:11<27:23,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:27<30:42,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:43<26:44,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [01:59<22:33,  1.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:15<17:06,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:32<13:54,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:48<14:53,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:04<09:19,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1202/1500 [03:21<06:21,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:37<05:55,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:53<02:54,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:04<00:00,  6.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for self-improve round 2.\n",
            "Evaluating 13-digit accuracy...\n",
            "Accuracy for 13 digits: 0.997\n",
            "Accuracy for 13 digits: 0.997\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 0.997\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Average accuracy for 13: 0.9987\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_2.pt\n",
            "This is round 2, data larger has 2050000 rows\n",
            "This is round 2, data smaller has 50000 rows\n",
            "2_round_combined_ds.txt has 2100000 rows\n",
            "Generating 3 si data...\n",
            "Already reached 50,000 lines. Stopping early.\n",
            "Writing complete. \n",
            "This filtered file has 0.026% wrong answer. \n",
            "This is round 3, The data used for training has 4.2 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:31:39,  6.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<42:29,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:38<39:26,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<29:58,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:11<26:47,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:28<30:08,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:44<24:07,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:01<25:50,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:17<16:38,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:33<18:03,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:49<14:59,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:06<12:02,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:22<09:07,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:39<06:31,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1402/1500 [03:55<02:07,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:05<00:00,  6.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for self-improve round 3.\n",
            "Evaluating 14-digit accuracy...\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 1.0\n",
            "Average accuracy for 14: 0.9994\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_3.pt\n",
            "This is round 3, data larger has 2100000 rows\n",
            "This is round 3, data smaller has 50000 rows\n",
            "3_round_combined_ds.txt has 2150000 rows\n",
            "Generating 4 si data...\n",
            "Already reached 50,000 lines. Stopping early.\n",
            "Writing complete. \n",
            "This filtered file has 0.03% wrong answer. \n",
            "This is round 4, The data used for training has 4.3 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:34:02,  6.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<42:33,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:39<39:24,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<36:11,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:11<25:53,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:28<30:09,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:44<27:08,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:00<23:30,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:17<21:09,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:34<17:50,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1002/1500 [02:50<10:35,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:07<13:06,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:23<09:03,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:40<05:59,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:56<03:12,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:07<00:00,  6.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for self-improve round 4.\n",
            "Evaluating 15-digit accuracy...\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Average accuracy for 15: 0.9998000000000001\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_4.pt\n",
            "This is round 4, data larger has 2150000 rows\n",
            "This is round 4, data smaller has 50000 rows\n",
            "4_round_combined_ds.txt has 2200000 rows\n",
            "Generating 5 si data...\n",
            "Already reached 50,000 lines. Stopping early.\n",
            "Writing complete. \n",
            "This filtered file has 0.032% wrong answer. \n",
            "This is round 5, The data used for training has 4.4 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:05<2:28:24,  5.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<40:11,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:38<38:54,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<32:03,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:11<27:44,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:27<29:25,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:43<27:08,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:00<23:36,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.1215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:16<16:16,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:33<18:14,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:49<15:13,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:06<11:58,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:22<09:05,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:39<06:02,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:56<02:49,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:06<00:00,  6.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for self-improve round 5.\n",
            "Evaluating 16-digit accuracy...\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.997\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.997\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Average accuracy for 16: 0.9987999999999999\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_5.pt\n",
            "This is round 5, data larger has 2200000 rows\n",
            "This is round 5, data smaller has 50000 rows\n",
            "5_round_combined_ds.txt has 2250000 rows\n",
            "Generating 6 si data...\n",
            "Already reached 50,000 lines. Stopping early.\n",
            "Writing complete. \n",
            "This filtered file has 0.026% wrong answer. \n",
            "This is round 6, The data used for training has 4.5 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:33:06,  6.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<42:27,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:38<39:34,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:54<31:41,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:11<29:56,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:28<28:15,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.1026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:44<27:09,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:01<25:09,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.1001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:17<20:40,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:33<18:04,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.1010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:50<15:24,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:07<11:46,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1202/1500 [03:23<06:24,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:39<05:59,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:56<02:58,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:07<00:00,  6.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for self-improve round 6.\n",
            "Evaluating 17-digit accuracy...\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Average accuracy for 17: 0.9997\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_6.pt\n",
            "This is round 6, data larger has 2250000 rows\n",
            "This is round 6, data smaller has 50000 rows\n",
            "6_round_combined_ds.txt has 2300000 rows\n",
            "Generating 7 si data...\n",
            "Already reached 50,000 lines. Stopping early.\n",
            "Writing complete. \n",
            "This filtered file has 0.018000000000000002% wrong answer. \n",
            "This is round 7, The data used for training has 4.6 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:32:25,  6.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<41:46,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:38<39:13,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<36:09,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:11<33:24,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:28<30:23,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:45<27:21,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.1026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:01<24:47,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:18<21:37,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:35<18:31,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:51<15:22,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:08<12:14,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:24<09:31,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:41<05:59,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:57<02:55,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:07<00:00,  6.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for self-improve round 7.\n",
            "Evaluating 18-digit accuracy...\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.998\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.997\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.998\n",
            "Average accuracy for 18: 0.9990999999999998\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_7.pt\n",
            "This is round 7, data larger has 2300000 rows\n",
            "This is round 7, data smaller has 50000 rows\n",
            "7_round_combined_ds.txt has 2350000 rows\n",
            "Generating 8 si data...\n",
            "Already reached 50,000 lines. Stopping early.\n",
            "Writing complete. \n",
            "This filtered file has 0.008% wrong answer. \n",
            "This is round 8, The data used for training has 4.7 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:32:05,  6.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<41:54,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:39<30:48,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.1087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<36:10,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.1016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:11<33:25,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:28<33:04,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:45<27:13,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:01<24:30,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:17<21:10,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:34<15:23,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1002/1500 [02:51<10:52,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.1042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:07<12:06,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:24<09:00,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.1076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:40<05:42,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:57<02:59,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:07<00:00,  6.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for self-improve round 8.\n",
            "Evaluating 19-digit accuracy...\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Average accuracy for 19: 0.9997\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_8.pt\n",
            "This is round 8, data larger has 2350000 rows\n",
            "This is round 8, data smaller has 50000 rows\n",
            "8_round_combined_ds.txt has 2400000 rows\n",
            "Generating 9 si data...\n",
            "Already reached 50,000 lines. Stopping early.\n",
            "Writing complete. \n",
            "This filtered file has 0.02% wrong answer. \n",
            "This is round 9, The data used for training has 4.8 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:33:18,  6.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<42:45,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:38<39:25,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<40:02,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:12<33:34,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.1138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:28<30:44,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.1046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:45<26:01,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:01<24:18,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:18<23:14,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:35<18:25,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.1015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:51<14:58,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:07<12:07,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:24<08:05,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:40<06:00,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:57<02:59,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:07<00:00,  6.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for self-improve round 9.\n",
            "Evaluating 20-digit accuracy...\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 0.998\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 0.999\n",
            "Average accuracy for 20: 0.9989000000000001\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_9.pt\n",
            "This is round 9, data larger has 2400000 rows\n",
            "This is round 9, data smaller has 50000 rows\n",
            "9_round_combined_ds.txt has 2450000 rows\n",
            "Generating 10 si data...\n",
            "Already reached 50,000 lines. Stopping early.\n",
            "Writing complete. \n",
            "This filtered file has 0.02% wrong answer. \n",
            "This is round 10, The data used for training has 4.9 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:33:04,  6.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<42:46,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:39<42:47,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.1156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<36:36,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.1037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:12<33:28,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.1176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:29<32:52,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:45<21:27,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:02<19:51,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:18<21:11,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:35<19:36,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:51<13:57,  1.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:08<12:06,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.1364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:24<08:41,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:41<05:54,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:57<02:38,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:07<00:00,  6.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for self-improve round 10.\n",
            "Evaluating 21-digit accuracy...\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 0.999\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 0.999\n",
            "Accuracy for 21 digits: 0.999\n",
            "Accuracy for 21 digits: 0.999\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 1.0\n",
            "Average accuracy for 21: 0.9995999999999998\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_10.pt\n",
            "This is round 10, data larger has 2450000 rows\n",
            "This is round 10, data smaller has 50000 rows\n",
            "10_round_combined_ds.txt has 2500000 rows\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>▃▁▅▇█▅█▆█▅▇</td></tr><tr><td>digit_step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>train_loss</td><td>▁▁▁▃▄▂▂▂▁▁▄▄▂▂▃▂▃▄▂▂▅▇▃▅▅▄▃▃▃▃█▃▃▅▃▄▄▅▃▆</td></tr><tr><td>train_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.9996</td></tr><tr><td>digit_step</td><td>21</td></tr><tr><td>train_loss</td><td>1.06616</td></tr><tr><td>train_step</td><td>149</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">si for 10 rounds with length filter</strong> at: <a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/rrp5vvvz' target=\"_blank\">https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/rrp5vvvz</a><br> View project at: <a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs' target=\"_blank\">https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250311_203102-rrp5vvvz/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"/content/drive/MyDrive/URPS/Data/si_data_r{6}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            # because 50% data from before, 50% from latest si data, copy (39+si_r) times\n",
        "            # Let both datasets have the same amount of data and then randomly draw from the combined dataset\n",
        "            sub_data = f.readlines()\n",
        "wrong = 0\n",
        "for i in range(len(sub_data)):\n",
        "    if sub_data[i][:(si_r+10)] != sub_data[i][(si_r+10+1): (si_r+10+1+si_r+10)]:\n",
        "          print(f\"SUB1:{sub_data[i][:(si_r+10)]}\")\n",
        "          print(f\"SUB2:{sub_data[i][(si_r+10+1): (si_r+10+1+si_r+10)]}\")\n",
        "          print(f\"{sub_data[i]}\")\n",
        "          break\n",
        "          wrong += 1\n",
        "    # if \"=\" not in sub_data[i]:\n",
        "    #     print(f\"Warning: Line {i} does not contain '=': {sub_data[i]}\")\n",
        "    #     continue  # 跳过这一行\n",
        "\n",
        "    # parts = sub_data[i].split(\"=\")\n",
        "\n",
        "    # if len(parts) != 2:\n",
        "    #     print(f\"Warning: Line {i} has multiple '=': {sub_data[i]}\")\n",
        "    #     continue  # 跳过这一行\n",
        "\n",
        "    # left_part, right_part = parts\n",
        "    # right_part = right_part.split(\"&\")[0]  # 去掉 `&` 及后面的部分\n",
        "\n",
        "    # if left_part[:(si_r+10)] != right_part[:(si_r+10)]:\n",
        "    #     print(f\"Mismatch found in line {i}: {left_part} != {right_part}\")\n",
        "print(f\"This filtered file has {(wrong / len(sub_data))*100}% wrong answer. \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oAYjOZyC2kMs",
        "outputId": "1ebf3d68-55b3-429e-a1b0-968ba5cb4fb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUB1:67862929989999128\n",
            "SUB2:67862929998999128\n",
            "67862929989999128=67862929998999128&\n",
            "\n",
            "This filtered file has 0.0% wrong answer. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/URPS/Data/si_data_r0.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.readlines()\n",
        "wrong = 0\n",
        "for i in range(len(data)):\n",
        "    if data[i][:11] != data[i][12:23]:\n",
        "        wrong +=1\n",
        "print(wrong / len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p57Ee9BdTpYf",
        "outputId": "67fea34e-4e7c-4f1f-f227-dd021a8316f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/URPS/Data/si1500steps/si_data_r0.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.readlines()\n",
        "wrong = 0\n",
        "for i in range(len(data)):\n",
        "    if data[i][:11] != data[i][12:23]:\n",
        "        wrong +=1\n",
        "print(wrong / len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR7_M4YPUetl",
        "outputId": "c89c50db-41b5-4bc3-bae1-a7c80a925c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff_model_performance = {}\n",
        "for i in range (11):\n",
        "    model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "    model.to(device)\n",
        "    checkpoint_path = f\"/content/drive/MyDrive/URPS/Models/sc_model_{i}.pt\"\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    one_list = []\n",
        "    for j in range(11, 22):\n",
        "        acc = test_accuracy_on_digits(model, j)\n",
        "        one_list.append(acc)\n",
        "    diff_model_performance[i] = one_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDfWl2Yoj2eg",
        "outputId": "28c80a1f-8d21-4daa-fb3c-315a06a14df1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-149-d0f2f32f3b41>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.997\n",
            "Accuracy for 11 digits: 0.997\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.997\n",
            "Accuracy for 11 digits: 0.997\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.998\n",
            "Accuracy for 11 digits: 0.996\n",
            "Accuracy for 11 digits: 0.994\n",
            "Accuracy for 12 digits: 0.879\n",
            "Accuracy for 12 digits: 0.87\n",
            "Accuracy for 12 digits: 0.873\n",
            "Accuracy for 12 digits: 0.858\n",
            "Accuracy for 12 digits: 0.877\n",
            "Accuracy for 12 digits: 0.887\n",
            "Accuracy for 12 digits: 0.89\n",
            "Accuracy for 12 digits: 0.884\n",
            "Accuracy for 12 digits: 0.888\n",
            "Accuracy for 12 digits: 0.882\n",
            "Accuracy for 13 digits: 0.365\n",
            "Accuracy for 13 digits: 0.346\n",
            "Accuracy for 13 digits: 0.327\n",
            "Accuracy for 13 digits: 0.356\n",
            "Accuracy for 13 digits: 0.362\n",
            "Accuracy for 13 digits: 0.359\n",
            "Accuracy for 13 digits: 0.357\n",
            "Accuracy for 13 digits: 0.355\n",
            "Accuracy for 13 digits: 0.344\n",
            "Accuracy for 13 digits: 0.325\n",
            "Accuracy for 14 digits: 0.037\n",
            "Accuracy for 14 digits: 0.035\n",
            "Accuracy for 14 digits: 0.035\n",
            "Accuracy for 14 digits: 0.028\n",
            "Accuracy for 14 digits: 0.033\n",
            "Accuracy for 14 digits: 0.032\n",
            "Accuracy for 14 digits: 0.029\n",
            "Accuracy for 14 digits: 0.024\n",
            "Accuracy for 14 digits: 0.032\n",
            "Accuracy for 14 digits: 0.026\n",
            "Accuracy for 15 digits: 0.0\n",
            "Accuracy for 15 digits: 0.001\n",
            "Accuracy for 15 digits: 0.0\n",
            "Accuracy for 15 digits: 0.0\n",
            "Accuracy for 15 digits: 0.0\n",
            "Accuracy for 15 digits: 0.0\n",
            "Accuracy for 15 digits: 0.0\n",
            "Accuracy for 15 digits: 0.001\n",
            "Accuracy for 15 digits: 0.0\n",
            "Accuracy for 15 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 0.996\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 13 digits: 0.981\n",
            "Accuracy for 13 digits: 0.966\n",
            "Accuracy for 13 digits: 0.964\n",
            "Accuracy for 13 digits: 0.962\n",
            "Accuracy for 13 digits: 0.975\n",
            "Accuracy for 13 digits: 0.967\n",
            "Accuracy for 13 digits: 0.961\n",
            "Accuracy for 13 digits: 0.97\n",
            "Accuracy for 13 digits: 0.959\n",
            "Accuracy for 13 digits: 0.958\n",
            "Accuracy for 14 digits: 0.722\n",
            "Accuracy for 14 digits: 0.725\n",
            "Accuracy for 14 digits: 0.718\n",
            "Accuracy for 14 digits: 0.722\n",
            "Accuracy for 14 digits: 0.714\n",
            "Accuracy for 14 digits: 0.726\n",
            "Accuracy for 14 digits: 0.721\n",
            "Accuracy for 14 digits: 0.704\n",
            "Accuracy for 14 digits: 0.702\n",
            "Accuracy for 14 digits: 0.703\n",
            "Accuracy for 15 digits: 0.235\n",
            "Accuracy for 15 digits: 0.259\n",
            "Accuracy for 15 digits: 0.239\n",
            "Accuracy for 15 digits: 0.229\n",
            "Accuracy for 15 digits: 0.25\n",
            "Accuracy for 15 digits: 0.245\n",
            "Accuracy for 15 digits: 0.212\n",
            "Accuracy for 15 digits: 0.235\n",
            "Accuracy for 15 digits: 0.239\n",
            "Accuracy for 15 digits: 0.216\n",
            "Accuracy for 16 digits: 0.027\n",
            "Accuracy for 16 digits: 0.019\n",
            "Accuracy for 16 digits: 0.018\n",
            "Accuracy for 16 digits: 0.027\n",
            "Accuracy for 16 digits: 0.032\n",
            "Accuracy for 16 digits: 0.023\n",
            "Accuracy for 16 digits: 0.021\n",
            "Accuracy for 16 digits: 0.021\n",
            "Accuracy for 16 digits: 0.026\n",
            "Accuracy for 16 digits: 0.025\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.001\n",
            "Accuracy for 17 digits: 0.001\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.001\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 14 digits: 0.99\n",
            "Accuracy for 14 digits: 0.988\n",
            "Accuracy for 14 digits: 0.989\n",
            "Accuracy for 14 digits: 0.987\n",
            "Accuracy for 14 digits: 0.984\n",
            "Accuracy for 14 digits: 0.986\n",
            "Accuracy for 14 digits: 0.996\n",
            "Accuracy for 14 digits: 0.988\n",
            "Accuracy for 14 digits: 0.977\n",
            "Accuracy for 14 digits: 0.991\n",
            "Accuracy for 15 digits: 0.919\n",
            "Accuracy for 15 digits: 0.916\n",
            "Accuracy for 15 digits: 0.918\n",
            "Accuracy for 15 digits: 0.914\n",
            "Accuracy for 15 digits: 0.906\n",
            "Accuracy for 15 digits: 0.923\n",
            "Accuracy for 15 digits: 0.913\n",
            "Accuracy for 15 digits: 0.935\n",
            "Accuracy for 15 digits: 0.905\n",
            "Accuracy for 15 digits: 0.913\n",
            "Accuracy for 16 digits: 0.71\n",
            "Accuracy for 16 digits: 0.751\n",
            "Accuracy for 16 digits: 0.738\n",
            "Accuracy for 16 digits: 0.717\n",
            "Accuracy for 16 digits: 0.707\n",
            "Accuracy for 16 digits: 0.727\n",
            "Accuracy for 16 digits: 0.723\n",
            "Accuracy for 16 digits: 0.724\n",
            "Accuracy for 16 digits: 0.725\n",
            "Accuracy for 16 digits: 0.725\n",
            "Accuracy for 17 digits: 0.402\n",
            "Accuracy for 17 digits: 0.422\n",
            "Accuracy for 17 digits: 0.392\n",
            "Accuracy for 17 digits: 0.403\n",
            "Accuracy for 17 digits: 0.4\n",
            "Accuracy for 17 digits: 0.409\n",
            "Accuracy for 17 digits: 0.411\n",
            "Accuracy for 17 digits: 0.4\n",
            "Accuracy for 17 digits: 0.372\n",
            "Accuracy for 17 digits: 0.389\n",
            "Accuracy for 18 digits: 0.103\n",
            "Accuracy for 18 digits: 0.114\n",
            "Accuracy for 18 digits: 0.118\n",
            "Accuracy for 18 digits: 0.102\n",
            "Accuracy for 18 digits: 0.128\n",
            "Accuracy for 18 digits: 0.125\n",
            "Accuracy for 18 digits: 0.117\n",
            "Accuracy for 18 digits: 0.125\n",
            "Accuracy for 18 digits: 0.114\n",
            "Accuracy for 18 digits: 0.129\n",
            "Accuracy for 19 digits: 0.005\n",
            "Accuracy for 19 digits: 0.01\n",
            "Accuracy for 19 digits: 0.011\n",
            "Accuracy for 19 digits: 0.009\n",
            "Accuracy for 19 digits: 0.008\n",
            "Accuracy for 19 digits: 0.013\n",
            "Accuracy for 19 digits: 0.016\n",
            "Accuracy for 19 digits: 0.012\n",
            "Accuracy for 19 digits: 0.011\n",
            "Accuracy for 19 digits: 0.013\n",
            "Accuracy for 20 digits: 0.001\n",
            "Accuracy for 20 digits: 0.001\n",
            "Accuracy for 20 digits: 0.001\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.001\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 21 digits: 0.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.997\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 0.998\n",
            "Accuracy for 15 digits: 0.998\n",
            "Accuracy for 15 digits: 0.996\n",
            "Accuracy for 15 digits: 0.997\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.997\n",
            "Accuracy for 15 digits: 0.998\n",
            "Accuracy for 15 digits: 0.997\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 16 digits: 0.982\n",
            "Accuracy for 16 digits: 0.988\n",
            "Accuracy for 16 digits: 0.989\n",
            "Accuracy for 16 digits: 0.991\n",
            "Accuracy for 16 digits: 0.99\n",
            "Accuracy for 16 digits: 0.987\n",
            "Accuracy for 16 digits: 0.989\n",
            "Accuracy for 16 digits: 0.992\n",
            "Accuracy for 16 digits: 0.98\n",
            "Accuracy for 16 digits: 0.983\n",
            "Accuracy for 17 digits: 0.935\n",
            "Accuracy for 17 digits: 0.953\n",
            "Accuracy for 17 digits: 0.952\n",
            "Accuracy for 17 digits: 0.922\n",
            "Accuracy for 17 digits: 0.94\n",
            "Accuracy for 17 digits: 0.947\n",
            "Accuracy for 17 digits: 0.933\n",
            "Accuracy for 17 digits: 0.94\n",
            "Accuracy for 17 digits: 0.936\n",
            "Accuracy for 17 digits: 0.927\n",
            "Accuracy for 18 digits: 0.803\n",
            "Accuracy for 18 digits: 0.757\n",
            "Accuracy for 18 digits: 0.771\n",
            "Accuracy for 18 digits: 0.79\n",
            "Accuracy for 18 digits: 0.795\n",
            "Accuracy for 18 digits: 0.765\n",
            "Accuracy for 18 digits: 0.788\n",
            "Accuracy for 18 digits: 0.775\n",
            "Accuracy for 18 digits: 0.785\n",
            "Accuracy for 18 digits: 0.803\n",
            "Accuracy for 19 digits: 0.48\n",
            "Accuracy for 19 digits: 0.516\n",
            "Accuracy for 19 digits: 0.492\n",
            "Accuracy for 19 digits: 0.51\n",
            "Accuracy for 19 digits: 0.489\n",
            "Accuracy for 19 digits: 0.529\n",
            "Accuracy for 19 digits: 0.465\n",
            "Accuracy for 19 digits: 0.489\n",
            "Accuracy for 19 digits: 0.486\n",
            "Accuracy for 19 digits: 0.513\n",
            "Accuracy for 20 digits: 0.153\n",
            "Accuracy for 20 digits: 0.151\n",
            "Accuracy for 20 digits: 0.158\n",
            "Accuracy for 20 digits: 0.155\n",
            "Accuracy for 20 digits: 0.152\n",
            "Accuracy for 20 digits: 0.17\n",
            "Accuracy for 20 digits: 0.157\n",
            "Accuracy for 20 digits: 0.172\n",
            "Accuracy for 20 digits: 0.168\n",
            "Accuracy for 20 digits: 0.166\n",
            "Accuracy for 21 digits: 0.027\n",
            "Accuracy for 21 digits: 0.024\n",
            "Accuracy for 21 digits: 0.024\n",
            "Accuracy for 21 digits: 0.015\n",
            "Accuracy for 21 digits: 0.033\n",
            "Accuracy for 21 digits: 0.024\n",
            "Accuracy for 21 digits: 0.028\n",
            "Accuracy for 21 digits: 0.021\n",
            "Accuracy for 21 digits: 0.017\n",
            "Accuracy for 21 digits: 0.026\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.997\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.997\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.998\n",
            "Accuracy for 16 digits: 0.987\n",
            "Accuracy for 16 digits: 0.983\n",
            "Accuracy for 16 digits: 0.99\n",
            "Accuracy for 16 digits: 0.993\n",
            "Accuracy for 16 digits: 0.993\n",
            "Accuracy for 16 digits: 0.997\n",
            "Accuracy for 16 digits: 0.99\n",
            "Accuracy for 16 digits: 0.99\n",
            "Accuracy for 16 digits: 0.989\n",
            "Accuracy for 16 digits: 0.996\n",
            "Accuracy for 17 digits: 0.929\n",
            "Accuracy for 17 digits: 0.927\n",
            "Accuracy for 17 digits: 0.931\n",
            "Accuracy for 17 digits: 0.911\n",
            "Accuracy for 17 digits: 0.934\n",
            "Accuracy for 17 digits: 0.933\n",
            "Accuracy for 17 digits: 0.924\n",
            "Accuracy for 17 digits: 0.933\n",
            "Accuracy for 17 digits: 0.94\n",
            "Accuracy for 17 digits: 0.936\n",
            "Accuracy for 18 digits: 0.74\n",
            "Accuracy for 18 digits: 0.741\n",
            "Accuracy for 18 digits: 0.741\n",
            "Accuracy for 18 digits: 0.747\n",
            "Accuracy for 18 digits: 0.759\n",
            "Accuracy for 18 digits: 0.764\n",
            "Accuracy for 18 digits: 0.748\n",
            "Accuracy for 18 digits: 0.743\n",
            "Accuracy for 18 digits: 0.721\n",
            "Accuracy for 18 digits: 0.765\n",
            "Accuracy for 19 digits: 0.351\n",
            "Accuracy for 19 digits: 0.398\n",
            "Accuracy for 19 digits: 0.367\n",
            "Accuracy for 19 digits: 0.378\n",
            "Accuracy for 19 digits: 0.403\n",
            "Accuracy for 19 digits: 0.396\n",
            "Accuracy for 19 digits: 0.363\n",
            "Accuracy for 19 digits: 0.397\n",
            "Accuracy for 19 digits: 0.378\n",
            "Accuracy for 19 digits: 0.39\n",
            "Accuracy for 20 digits: 0.091\n",
            "Accuracy for 20 digits: 0.091\n",
            "Accuracy for 20 digits: 0.096\n",
            "Accuracy for 20 digits: 0.089\n",
            "Accuracy for 20 digits: 0.096\n",
            "Accuracy for 20 digits: 0.094\n",
            "Accuracy for 20 digits: 0.086\n",
            "Accuracy for 20 digits: 0.087\n",
            "Accuracy for 20 digits: 0.105\n",
            "Accuracy for 20 digits: 0.097\n",
            "Accuracy for 21 digits: 0.013\n",
            "Accuracy for 21 digits: 0.013\n",
            "Accuracy for 21 digits: 0.009\n",
            "Accuracy for 21 digits: 0.012\n",
            "Accuracy for 21 digits: 0.008\n",
            "Accuracy for 21 digits: 0.008\n",
            "Accuracy for 21 digits: 0.011\n",
            "Accuracy for 21 digits: 0.005\n",
            "Accuracy for 21 digits: 0.008\n",
            "Accuracy for 21 digits: 0.008\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 17 digits: 0.984\n",
            "Accuracy for 17 digits: 0.987\n",
            "Accuracy for 17 digits: 0.991\n",
            "Accuracy for 17 digits: 0.984\n",
            "Accuracy for 17 digits: 0.991\n",
            "Accuracy for 17 digits: 0.991\n",
            "Accuracy for 17 digits: 0.991\n",
            "Accuracy for 17 digits: 0.993\n",
            "Accuracy for 17 digits: 0.984\n",
            "Accuracy for 17 digits: 0.988\n",
            "Accuracy for 18 digits: 0.955\n",
            "Accuracy for 18 digits: 0.94\n",
            "Accuracy for 18 digits: 0.933\n",
            "Accuracy for 18 digits: 0.934\n",
            "Accuracy for 18 digits: 0.935\n",
            "Accuracy for 18 digits: 0.928\n",
            "Accuracy for 18 digits: 0.936\n",
            "Accuracy for 18 digits: 0.939\n",
            "Accuracy for 18 digits: 0.952\n",
            "Accuracy for 18 digits: 0.941\n",
            "Accuracy for 19 digits: 0.805\n",
            "Accuracy for 19 digits: 0.788\n",
            "Accuracy for 19 digits: 0.812\n",
            "Accuracy for 19 digits: 0.813\n",
            "Accuracy for 19 digits: 0.807\n",
            "Accuracy for 19 digits: 0.789\n",
            "Accuracy for 19 digits: 0.775\n",
            "Accuracy for 19 digits: 0.814\n",
            "Accuracy for 19 digits: 0.796\n",
            "Accuracy for 19 digits: 0.81\n",
            "Accuracy for 20 digits: 0.528\n",
            "Accuracy for 20 digits: 0.528\n",
            "Accuracy for 20 digits: 0.516\n",
            "Accuracy for 20 digits: 0.538\n",
            "Accuracy for 20 digits: 0.53\n",
            "Accuracy for 20 digits: 0.498\n",
            "Accuracy for 20 digits: 0.541\n",
            "Accuracy for 20 digits: 0.499\n",
            "Accuracy for 20 digits: 0.556\n",
            "Accuracy for 20 digits: 0.544\n",
            "Accuracy for 21 digits: 0.218\n",
            "Accuracy for 21 digits: 0.206\n",
            "Accuracy for 21 digits: 0.191\n",
            "Accuracy for 21 digits: 0.207\n",
            "Accuracy for 21 digits: 0.216\n",
            "Accuracy for 21 digits: 0.214\n",
            "Accuracy for 21 digits: 0.211\n",
            "Accuracy for 21 digits: 0.212\n",
            "Accuracy for 21 digits: 0.201\n",
            "Accuracy for 21 digits: 0.185\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 18 digits: 0.993\n",
            "Accuracy for 18 digits: 0.995\n",
            "Accuracy for 18 digits: 0.993\n",
            "Accuracy for 18 digits: 0.993\n",
            "Accuracy for 18 digits: 0.99\n",
            "Accuracy for 18 digits: 0.99\n",
            "Accuracy for 18 digits: 0.99\n",
            "Accuracy for 18 digits: 0.989\n",
            "Accuracy for 18 digits: 0.99\n",
            "Accuracy for 18 digits: 0.994\n",
            "Accuracy for 19 digits: 0.907\n",
            "Accuracy for 19 digits: 0.899\n",
            "Accuracy for 19 digits: 0.911\n",
            "Accuracy for 19 digits: 0.909\n",
            "Accuracy for 19 digits: 0.905\n",
            "Accuracy for 19 digits: 0.903\n",
            "Accuracy for 19 digits: 0.892\n",
            "Accuracy for 19 digits: 0.889\n",
            "Accuracy for 19 digits: 0.898\n",
            "Accuracy for 19 digits: 0.905\n",
            "Accuracy for 20 digits: 0.619\n",
            "Accuracy for 20 digits: 0.607\n",
            "Accuracy for 20 digits: 0.582\n",
            "Accuracy for 20 digits: 0.589\n",
            "Accuracy for 20 digits: 0.608\n",
            "Accuracy for 20 digits: 0.611\n",
            "Accuracy for 20 digits: 0.599\n",
            "Accuracy for 20 digits: 0.594\n",
            "Accuracy for 20 digits: 0.617\n",
            "Accuracy for 20 digits: 0.603\n",
            "Accuracy for 21 digits: 0.226\n",
            "Accuracy for 21 digits: 0.197\n",
            "Accuracy for 21 digits: 0.228\n",
            "Accuracy for 21 digits: 0.227\n",
            "Accuracy for 21 digits: 0.2\n",
            "Accuracy for 21 digits: 0.226\n",
            "Accuracy for 21 digits: 0.216\n",
            "Accuracy for 21 digits: 0.246\n",
            "Accuracy for 21 digits: 0.191\n",
            "Accuracy for 21 digits: 0.208\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 18 digits: 0.998\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.996\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 19 digits: 0.989\n",
            "Accuracy for 19 digits: 0.995\n",
            "Accuracy for 19 digits: 0.995\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.995\n",
            "Accuracy for 19 digits: 0.993\n",
            "Accuracy for 19 digits: 0.997\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.997\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 20 digits: 0.988\n",
            "Accuracy for 20 digits: 0.984\n",
            "Accuracy for 20 digits: 0.972\n",
            "Accuracy for 20 digits: 0.983\n",
            "Accuracy for 20 digits: 0.983\n",
            "Accuracy for 20 digits: 0.98\n",
            "Accuracy for 20 digits: 0.981\n",
            "Accuracy for 20 digits: 0.979\n",
            "Accuracy for 20 digits: 0.984\n",
            "Accuracy for 20 digits: 0.976\n",
            "Accuracy for 21 digits: 0.924\n",
            "Accuracy for 21 digits: 0.934\n",
            "Accuracy for 21 digits: 0.926\n",
            "Accuracy for 21 digits: 0.92\n",
            "Accuracy for 21 digits: 0.921\n",
            "Accuracy for 21 digits: 0.93\n",
            "Accuracy for 21 digits: 0.943\n",
            "Accuracy for 21 digits: 0.928\n",
            "Accuracy for 21 digits: 0.934\n",
            "Accuracy for 21 digits: 0.933\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 20 digits: 0.995\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 0.995\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.996\n",
            "Accuracy for 21 digits: 0.987\n",
            "Accuracy for 21 digits: 0.978\n",
            "Accuracy for 21 digits: 0.984\n",
            "Accuracy for 21 digits: 0.983\n",
            "Accuracy for 21 digits: 0.984\n",
            "Accuracy for 21 digits: 0.985\n",
            "Accuracy for 21 digits: 0.984\n",
            "Accuracy for 21 digits: 0.983\n",
            "Accuracy for 21 digits: 0.983\n",
            "Accuracy for 21 digits: 0.978\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 21 digits: 0.984\n",
            "Accuracy for 21 digits: 0.996\n",
            "Accuracy for 21 digits: 0.994\n",
            "Accuracy for 21 digits: 0.996\n",
            "Accuracy for 21 digits: 0.997\n",
            "Accuracy for 21 digits: 0.998\n",
            "Accuracy for 21 digits: 0.996\n",
            "Accuracy for 21 digits: 0.99\n",
            "Accuracy for 21 digits: 0.993\n",
            "Accuracy for 21 digits: 0.989\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 0.999\n",
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 0.999\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 0.999\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 1.0\n",
            "Accuracy for 21 digits: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "x_values = [i for i in range(11, 22)]\n",
        "\n",
        "\n",
        "i = 0\n",
        "for m_performace in diff_model_performance.values():\n",
        "    fig.add_trace(go.Scatter(x=x_values,\n",
        "                             y=m_performace,\n",
        "                             mode='lines+markers',\n",
        "                             name=f\"SI for different model{i} in 1500 steps\"))\n",
        "    i += 1\n",
        "\n",
        "fig.update_layout(title=\"Comparison of Accuracy os SI for different models in 1500 steps with length filter\", xaxis_title=\"number of digits\", yaxis_title=\"Average Accuracy\")\n",
        "fig.update_layout(xaxis_title=\"number of digits\", yaxis_title=\"Average Accuracy\")\n",
        "fig.update_yaxes(range=[-0.02, 1.02])\n",
        "fig.update_xaxes(tickmode=\"array\", tickvals=x_values)\n",
        "fig.update_layout(width=1000, height=500)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "wandb.init(project=\"transformer_si_graphs\", name=\"si for 10 rounds with length filter\")\n",
        "wandb.log({\"Interactive Chart\": wandb.Html(fig.to_html())})\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "7Ihomw5i40Tg",
        "outputId": "559bdb8d-d8fb-4558-d22e-f1951d59fec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"a95f716f-573c-429d-8802-2d7d5c755797\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a95f716f-573c-429d-8802-2d7d5c755797\")) {                    Plotly.newPlot(                        \"a95f716f-573c-429d-8802-2d7d5c755797\",                        [{\"mode\":\"lines+markers\",\"name\":\"SI for different model0 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[0.9974999999999999,0.8788,0.3496,0.031100000000000006,0.0002,0.0,0.0,0.0,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"SI for different model1 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[1.0,0.9984,0.9663,0.7157,0.2359,0.023899999999999998,0.00030000000000000003,0.0,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"SI for different model2 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[1.0,1.0,0.9992999999999999,0.9875999999999999,0.9162000000000001,0.7246999999999999,0.4,0.11750000000000001,0.010799999999999999,0.0004,0.0],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"SI for different model3 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[1.0,0.9999,0.9996,0.9992999999999999,0.9979000000000001,0.9871000000000001,0.9385,0.7832000000000001,0.49689999999999995,0.16019999999999998,0.0239],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"SI for different model4 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[0.9999,0.9997,0.9998000000000001,0.9999,0.9991,0.9908000000000001,0.9298,0.7469,0.3821,0.09319999999999998,0.0095],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"SI for different model5 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[0.9998000000000001,1.0,0.9999,0.9999,0.9997,0.9991,0.9884000000000001,0.9393000000000002,0.8009000000000001,0.5278,0.2061],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"SI for different model6 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[0.9998999999999999,0.9998999999999999,0.9998000000000001,0.9996,0.9999,0.9994999999999999,0.9993000000000001,0.9917,0.9018,0.6029,0.2165],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"SI for different model7 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[1.0,1.0,0.9997999999999999,1.0,0.9998999999999999,0.9999,0.9998999999999999,0.9991,0.9958,0.9809999999999999,0.9292999999999999],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"SI for different model8 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[0.9999,0.9998000000000001,0.9998999999999999,0.9999,0.9999,0.9995,0.9994999999999999,0.9997999999999999,0.9993000000000001,0.9968999999999999,0.9829000000000001],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"SI for different model9 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[0.9998000000000001,1.0,0.9997999999999999,1.0,1.0,0.9996,1.0,1.0,1.0,0.9994999999999999,0.9933000000000002],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"SI for different model10 in 1500 steps\",\"x\":[11,12,13,14,15,16,17,18,19,20,21],\"y\":[0.9999,0.9998999999999999,0.9996,0.9999,0.9998000000000001,0.9998999999999999,1.0,0.9999,0.9995999999999998,0.9997,0.9998000000000001],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Comparison of Accuracy os SI for different models in 1500 steps with length filter\"},\"xaxis\":{\"title\":{\"text\":\"number of digits\"},\"tickmode\":\"array\",\"tickvals\":[11,12,13,14,15,16,17,18,19,20,21]},\"yaxis\":{\"title\":{\"text\":\"Average Accuracy\"},\"range\":[-0.02,1.02]},\"width\":1000,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a95f716f-573c-429d-8802-2d7d5c755797');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250311_214436-ewgxsag0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/ewgxsag0' target=\"_blank\">si for 10 rounds with length filter</a></strong> to <a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs' target=\"_blank\">https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/ewgxsag0' target=\"_blank\">https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/ewgxsag0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">si for 10 rounds with length filter</strong> at: <a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/ewgxsag0' target=\"_blank\">https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs/runs/ewgxsag0</a><br> View project at: <a href='https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs' target=\"_blank\">https://wandb.ai/birdyyybai-university-of-michigan/transformer_si_graphs</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250311_214436-ewgxsag0/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}